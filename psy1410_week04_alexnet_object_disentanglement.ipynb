{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "psy1410_week04_alexnet_object_disentanglement.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPpumnE7I1z+NBHB/Ojarh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/psy1410/blob/master/psy1410_week04_alexnet_object_disentanglement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRYutqc7NLDL"
      },
      "source": [
        "# Week04 | Workshop on Object Representation\n",
        "\n",
        "Today we're going to play with AlexNet! \n",
        "\n",
        "First, we're going to explore the extent to which AlexNet features at different stages \"disentangle\" object categories for a subset of imagenet categories.\n",
        "\n",
        "Second, you'll either: (a) perform the same analyses on a different dataset, or (b) perform the same analyses with a different vision model. The vision model can have a different architecture (\"resnet50\"), or a different task (\"object detection\" instead of imagenet classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONTC1e0PL47"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOOdKqr5PNMn"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJL1VRkFbRxO"
      },
      "source": [
        "%%html\n",
        "<style>\n",
        "    .jp-Stdin-input {\n",
        "        width: 90%;\n",
        "    }\n",
        "    \n",
        "    div.p-Widget.jp-RenderedHTMLCommon.jp-RenderedMarkdown.jp-MarkdownOutput p {\n",
        "      font-size: 16px\n",
        "    }\n",
        "    a\n",
        "</style>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRr9Ass1PKCE"
      },
      "source": [
        "from torchvision import models, datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import seaborn as sns \n",
        "from scipy.stats import pearsonr \n",
        "\n",
        "def show_kernels(model):\n",
        "  if hasattr(model, 'features'):\n",
        "    kernels = model.features[0].weight.detach().clone().cpu()\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    img = make_grid(kernels, nrow=16)\n",
        "    plt.imshow(img.permute(1, 2, 0))  \n",
        "\n",
        "def show_images(dataset, num_categories=10, num_per_category=5):\n",
        "  targets = np.array(dataset.targets)\n",
        "  show_targets = np.unique(targets)[0:num_categories]\n",
        "  imgs = []\n",
        "  for target in show_targets:\n",
        "    indexes = np.where(targets == target)[0][0:num_per_category]\n",
        "    for idx in indexes:\n",
        "      img,label = dataset[idx]\n",
        "      imgs.append((np.array(img), label))\n",
        "\n",
        "  fig = plt.figure(figsize=(5*num_categories, 5*num_per_category))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(num_categories, num_per_category),  # creates 2x2 grid of axes\n",
        "                  axes_pad=0.5,  # pad between axes in inch.\n",
        "                  )\n",
        "\n",
        "  for index, (ax, (img,label)) in enumerate(zip(grid, imgs)):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
        "      ax.set_title(f'label={label}', fontsize=16)\n",
        "  for ax in grid: ax.axis('off')\n",
        "      \n",
        "  plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2h3HlNrlo4Y"
      },
      "source": [
        "'''\n",
        "Utilities for visualizing feature entanglement/disentanglement.\n",
        "'''\n",
        "import math \n",
        "import seaborn as sns \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from scipy.stats import pearsonr \n",
        "from fastprogress.fastprogress import master_bar, progress_bar \n",
        "from IPython.core.debugger import set_trace \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn import manifold\n",
        "from scipy.spatial.distance import pdist\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Optional, Union, List, Dict, Callable\n",
        "\n",
        "def mds_plot(layer_name, pairwise_similarity, labels, ax=None):\n",
        "  if ax is None:\n",
        "    ax = plt.gca()\n",
        "\n",
        "  ax.set_title(f'{layer_name}')\n",
        "\n",
        "  RDM = 1 - pairwise_similarity[layer_name]\n",
        "  n_components = 2\n",
        "  random_seed = None\n",
        "  is_metric = True \n",
        "  max_iter = 3000\n",
        "  convergence_tolerance = 1e-9\n",
        "\n",
        "  seed = None if random_seed is None else np.random.RandomState(seed=random_seed)\n",
        "  # for whatever reason, MDS only allows dissimilarity='precomputed' or dissimilarity='euclidean'\n",
        "  # dissimilarities = squareform(pdist(data_df.values, distance))\n",
        "  # Note: n_jobs>1 hangs indefinitely\n",
        "  mds = manifold.MDS(n_components=n_components, metric=is_metric, max_iter=max_iter, eps=convergence_tolerance, random_state=seed, dissimilarity='precomputed')\n",
        "  out = mds.fit_transform(RDM)\n",
        "  num_labels = len(labels.unique())\n",
        "  colorize = dict(c=labels, cmap=plt.cm.get_cmap('rainbow', num_labels))\n",
        "  ax.scatter(out[:, 0], out[:, 1], **colorize)\n",
        "  ax.axis('square');\n",
        "  ax.set_xlim([-1,1]);\n",
        "  ax.set_ylim([-1,1]);\n",
        "\n",
        "def show_mds_plots(pairwise_similarity, labels, ncols=3):\n",
        "  layer_names = pairwise_similarity.keys()\n",
        "  num_layers = len(layer_names)\n",
        "  nrows = math.ceil(num_layers/ncols)\n",
        "  fig, axes = plt.subplots(nrows, ncols, sharex=True, figsize=(ncols*4,nrows*4))\n",
        "  fig.suptitle('MDS plots show the \"position\" of each image in each feature space')\n",
        "  \n",
        "  for ax,layer_name in progress_bar(zip(axes, layer_names), total=num_layers):\n",
        "    mds_plot(layer_name, pairwise_similarity, labels, ax=ax)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "702GTB0qQ1QO"
      },
      "source": [
        "'''\n",
        "Utilities for instrumenting a torch model.\n",
        "\n",
        "InstrumentedModel will wrap a pytorch model and allow hooking\n",
        "arbitrary layers to monitor or modify their output directly.\n",
        "'''\n",
        "\n",
        "import torch, numpy, types, copy\n",
        "from collections import OrderedDict, defaultdict\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "class InstrumentedModel(torch.nn.Module):\n",
        "    '''\n",
        "    A wrapper for hooking, probing and intervening in pytorch Modules.\n",
        "    Example usage:\n",
        "\n",
        "    ```\n",
        "    model = load_my_model()\n",
        "    with inst as InstrumentedModel(model):\n",
        "        inst.retain_layer(layername)\n",
        "        inst.edit_layer(layername, ablation=0.5, replacement=target_features)\n",
        "        inst(inputs)\n",
        "        original_features = inst.retained_layer(layername)\n",
        "    ```\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self._retained = OrderedDict()\n",
        "        self._detach_retained = {}\n",
        "        self._editargs = defaultdict(dict)\n",
        "        self._editrule = {}\n",
        "        self._hooked_layer = {}\n",
        "        self._old_forward = {}\n",
        "        if isinstance(model, torch.nn.Sequential):\n",
        "            self._hook_sequential()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.close()\n",
        "\n",
        "    def forward(self, *inputs, **kwargs):\n",
        "        return self.model(*inputs, **kwargs)\n",
        "\n",
        "    def retain_layer(self, layername, detach=True):\n",
        "        '''\n",
        "        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n",
        "        to hook that layer and retain its output each time the model is run.\n",
        "        A pair (layername, aka) can be provided, and the aka will be used\n",
        "        as the key for the retained value instead of the layername.\n",
        "        '''\n",
        "        self.retain_layers([layername], detach=detach)\n",
        "\n",
        "    def retain_layers(self, layernames, detach=True):\n",
        "        '''\n",
        "        Retains a list of a layers at once.\n",
        "        '''\n",
        "        self.add_hooks(layernames)\n",
        "        for layername in layernames:\n",
        "            aka = layername\n",
        "            if not isinstance(aka, str):\n",
        "                layername, aka = layername\n",
        "            if aka not in self._retained:\n",
        "                self._retained[aka] = None\n",
        "                self._detach_retained[aka] = detach\n",
        "\n",
        "    def stop_retaining_layers(self, layernames):\n",
        "        '''\n",
        "        Removes a list of layers from the set retained.\n",
        "        '''\n",
        "        self.add_hooks(layernames)\n",
        "        for layername in layernames:\n",
        "            aka = layername\n",
        "            if not isinstance(aka, str):\n",
        "                layername, aka = layername\n",
        "            if aka in self._retained:\n",
        "                del self._retained[aka]\n",
        "                del self._detach_retained[aka]\n",
        "\n",
        "    def retained_features(self, clear=False):\n",
        "        '''\n",
        "        Returns a dict of all currently retained features.\n",
        "        '''\n",
        "        result = OrderedDict(self._retained)\n",
        "        if clear:\n",
        "            for k in result:\n",
        "                self._retained[k] = None\n",
        "        return result\n",
        "\n",
        "    def retained_layer(self, aka=None, clear=False):\n",
        "        '''\n",
        "        Retrieve retained data that was previously hooked by retain_layer.\n",
        "        Call this after the model is run.  If clear is set, then the\n",
        "        retained value will return and also cleared.\n",
        "        '''\n",
        "        if aka is None:\n",
        "            # Default to the first retained layer.\n",
        "            aka = next(self._retained.keys().__iter__())\n",
        "        result = self._retained[aka]\n",
        "        if clear:\n",
        "            self._retained[aka] = None\n",
        "        return result\n",
        "\n",
        "    def edit_layer(self, layername, rule=None, **kwargs):\n",
        "        '''\n",
        "        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n",
        "        to hook that layer and modify its output each time the model is run.\n",
        "        The output of the layer will be modified to be a convex combination\n",
        "        of the replacement and x interpolated according to the ablation, i.e.:\n",
        "        `output = x * (1 - a) + (r * a)`.\n",
        "        '''\n",
        "        if not isinstance(layername, str):\n",
        "            layername, aka = layername\n",
        "        else:\n",
        "            aka = layername\n",
        "\n",
        "        # The default editing rule is apply_ablation_replacement\n",
        "        if rule is None:\n",
        "            rule = apply_ablation_replacement\n",
        "\n",
        "        self.add_hooks([(layername, aka)])\n",
        "        self._editargs[aka].update(kwargs)\n",
        "        self._editrule[aka] = rule\n",
        "\n",
        "    def remove_edits(self, layername=None):\n",
        "        '''\n",
        "        Removes edits at the specified layer, or removes edits at all layers\n",
        "        if no layer name is specified.\n",
        "        '''\n",
        "        if layername is None:\n",
        "            self._editargs.clear()\n",
        "            self._editrule.clear()\n",
        "            return\n",
        "\n",
        "        if not isinstance(layername, str):\n",
        "            layername, aka = layername\n",
        "        else:\n",
        "            aka = layername\n",
        "        if aka in self._editargs:\n",
        "            del self._editargs[aka]\n",
        "        if aka in self._editrule:\n",
        "            del self._editrule[aka]\n",
        "\n",
        "    def add_hooks(self, layernames):\n",
        "        '''\n",
        "        Sets up a set of layers to be hooked.\n",
        "\n",
        "        Usually not called directly: use edit_layer or retain_layer instead.\n",
        "        '''\n",
        "        needed = set()\n",
        "        aka_map = {}\n",
        "        for name in layernames:\n",
        "            aka = name\n",
        "            if not isinstance(aka, str):\n",
        "                name, aka = name\n",
        "            if self._hooked_layer.get(aka, None) != name:\n",
        "                aka_map[name] = aka\n",
        "                needed.add(name)\n",
        "        if not needed:\n",
        "            return\n",
        "        for name, layer in self.model.named_modules():\n",
        "            if name in aka_map:\n",
        "                needed.remove(name)\n",
        "                aka = aka_map[name]\n",
        "                self._hook_layer(layer, name, aka)\n",
        "        for name in needed:\n",
        "            raise ValueError('Layer %s not found in model' % name)\n",
        "\n",
        "    def _hook_layer(self, layer, layername, aka):\n",
        "        '''\n",
        "        Internal method to replace a forward method with a closure that\n",
        "        intercepts the call, and tracks the hook so that it can be reverted.\n",
        "        '''\n",
        "        if aka in self._hooked_layer:\n",
        "            raise ValueError('Layer %s already hooked' % aka)\n",
        "        if layername in self._old_forward:\n",
        "            raise ValueError('Layer %s already hooked' % layername)\n",
        "        self._hooked_layer[aka] = layername\n",
        "        self._old_forward[layername] = (layer, aka,\n",
        "                layer.__dict__.get('forward', None))\n",
        "        editor = self\n",
        "        original_forward = layer.forward\n",
        "        def new_forward(self, *inputs, **kwargs):\n",
        "            original_x = original_forward(*inputs, **kwargs)\n",
        "            x = editor._postprocess_forward(original_x, aka)\n",
        "            return x\n",
        "        layer.forward = types.MethodType(new_forward, layer)\n",
        "\n",
        "    def _unhook_layer(self, aka):\n",
        "        '''\n",
        "        Internal method to remove a hook, restoring the original forward method.\n",
        "        '''\n",
        "        if aka not in self._hooked_layer:\n",
        "            return\n",
        "        layername = self._hooked_layer[aka]\n",
        "        # Remove any retained data and any edit rules\n",
        "        if aka in self._retained:\n",
        "            del self._retained[aka]\n",
        "            del self._detach_retained[aka]\n",
        "        self.remove_edits(aka)\n",
        "        # Restore the unhooked method for the layer\n",
        "        layer, check, old_forward = self._old_forward[layername]\n",
        "        assert check == aka\n",
        "        if old_forward is None:\n",
        "            if 'forward' in layer.__dict__:\n",
        "                del layer.__dict__['forward']\n",
        "        else:\n",
        "            layer.forward = old_forward\n",
        "        del self._old_forward[layername]\n",
        "        del self._hooked_layer[aka]\n",
        "\n",
        "    def _postprocess_forward(self, x, aka):\n",
        "        '''\n",
        "        The internal method called by the hooked layers after they are run.\n",
        "        '''\n",
        "        # Retain output before edits, if desired.\n",
        "        if aka in self._retained:\n",
        "            if self._detach_retained[aka]:\n",
        "                self._retained[aka] = x.detach()\n",
        "            else:\n",
        "                self._retained[aka] = x\n",
        "        # Apply any edits requested.\n",
        "        rule = self._editrule.get(aka, None)\n",
        "        if rule is not None:\n",
        "            x = rule(x, self, **(self._editargs[aka]))\n",
        "        return x\n",
        "\n",
        "    def _hook_sequential(self):\n",
        "        '''\n",
        "        Replaces 'forward' of sequential with a version that takes\n",
        "        additional keyword arguments: layer allows a single layer to be run;\n",
        "        first_layer and last_layer allow a subsequence of layers to be run.\n",
        "        '''\n",
        "        model = self.model\n",
        "        self._hooked_layer['.'] = '.'\n",
        "        self._old_forward['.'] = (model, '.',\n",
        "                model.__dict__.get('forward', None))\n",
        "        def new_forward(this, x, layer=None, first_layer=None, last_layer=None):\n",
        "            assert layer is None or (first_layer is None and last_layer is None)\n",
        "            first_layer, last_layer = [str(layer) if layer is not None\n",
        "                    else str(d) if d is not None else None\n",
        "                    for d in [first_layer, last_layer]]\n",
        "            including_children = (first_layer is None)\n",
        "            for name, layer in this._modules.items():\n",
        "                if name == first_layer:\n",
        "                    first_layer = None\n",
        "                    including_children = True\n",
        "                if including_children:\n",
        "                    x = layer(x)\n",
        "                if name == last_layer:\n",
        "                    last_layer = None\n",
        "                    including_children = False\n",
        "            assert first_layer is None, '%s not found' % first_layer\n",
        "            assert last_layer is None, '%s not found' % last_layer\n",
        "            return x\n",
        "        model.forward = types.MethodType(new_forward, model)\n",
        "\n",
        "    def close(self):\n",
        "        '''\n",
        "        Unhooks all hooked layers in the model.\n",
        "        '''\n",
        "        for aka in list(self._old_forward.keys()):\n",
        "            self._unhook_layer(aka)\n",
        "        assert len(self._old_forward) == 0\n",
        "\n",
        "def apply_ablation_replacement(x, imodel, **buffers):\n",
        "    if buffers is not None:\n",
        "        # Apply any edits requested.\n",
        "        a = make_matching_tensor(buffers, 'ablation', x)\n",
        "        if a is not None:\n",
        "            x = x * (1 - a)\n",
        "            v = make_matching_tensor(buffers, 'replacement', x)\n",
        "            if v is not None:\n",
        "                x += (v * a)\n",
        "    return x\n",
        "\n",
        "def make_matching_tensor(valuedict, name, data):\n",
        "    '''\n",
        "    Converts `valuedict[name]` to be a tensor with the same dtype, device,\n",
        "    and dimension count as `data`, and caches the converted tensor.\n",
        "    '''\n",
        "    v = valuedict.get(name, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    if not isinstance(v, torch.Tensor):\n",
        "        # Accept non-torch data.\n",
        "        v = torch.from_numpy(numpy.array(v))\n",
        "        valuedict[name] = v\n",
        "    if not v.device == data.device or not v.dtype == data.dtype:\n",
        "        # Ensure device and type matches.\n",
        "        assert not v.requires_grad, '%s wrong device or type' % (name)\n",
        "        v = v.to(device=data.device, dtype=data.dtype)\n",
        "        valuedict[name] = v\n",
        "    if len(v.shape) < len(data.shape):\n",
        "        # Ensure dimensions are unsqueezed as needed.\n",
        "        assert not v.requires_grad, '%s wrong dimensions' % (name)\n",
        "        v = v.view((1,) + tuple(v.shape) +\n",
        "                (1,) * (len(data.shape) - len(v.shape) - 1))\n",
        "        valuedict[name] = v\n",
        "    return v\n",
        "\n",
        "def subsequence(sequential, first_layer=None, last_layer=None,\n",
        "            share_weights=False):\n",
        "    '''\n",
        "    Creates a subsequence of a pytorch Sequential model, copying over\n",
        "    modules together with parameters for the subsequence.  Only\n",
        "    modules from first_layer to last_layer (inclusive) are included.\n",
        "\n",
        "    If share_weights is True, then references the original modules\n",
        "    and their parameters without copying them.  Otherwise, by default,\n",
        "    makes a separate brand-new copy.\n",
        "    '''\n",
        "    included_children = OrderedDict()\n",
        "    including_children = (first_layer is None)\n",
        "    for name, layer in sequential._modules.items():\n",
        "        if name == first_layer:\n",
        "            first_layer = None\n",
        "            including_children = True\n",
        "        if including_children:\n",
        "            included_children[name] = layer if share_weights else (\n",
        "                    copy.deepcopy(layer))\n",
        "        if name == last_layer:\n",
        "            last_layer = None\n",
        "            including_children = False\n",
        "    if first_layer is not None:\n",
        "        raise ValueError('Layer %s not found' % first_layer)\n",
        "    if last_layer is not None:\n",
        "        raise ValueError('Layer %s not found' % last_layer)\n",
        "    if not len(included_children):\n",
        "        raise ValueError('Empty subsequence')\n",
        "    return torch.nn.Sequential(OrderedDict(included_children))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhKs5qDPpyKF"
      },
      "source": [
        "from collections import OrderedDict \n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from IPython.core.debugger import set_trace \n",
        "\n",
        "def get_features(model, dataloader, layer_names):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  if not isinstance(model, InstrumentedModel):\n",
        "    model = InstrumentedModel(model)\n",
        "\n",
        "  if isinstance(layer_names, str):\n",
        "    layer_names = [layer_names]\n",
        "\n",
        "  model.retain_layers(layer_names)\n",
        "\n",
        "  pairwise_similarity = OrderedDict({})\n",
        "  features = OrderedDict({})\n",
        "  labels = []\n",
        "    \n",
        "  mb = master_bar(dataloader)\n",
        "  for batch in mb:\n",
        "    imgs = batch[0].to(device)\n",
        "    targets = batch[1].to(device)\n",
        "    batch_size = imgs.shape[0]\n",
        "    labels.append(targets)\n",
        "\n",
        "    if 'pixels' not in features:\n",
        "      features['pixels'] = []\n",
        "    features['pixels'].append(imgs.view(batch_size, -1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model(imgs)\n",
        "\n",
        "    for layer_name in progress_bar(layer_names, parent=mb):      \n",
        "      X = model.retained_layer(layer_name)\n",
        "      #if len(X.shape) == 4:\n",
        "      #  X = X.sum(dim=1)\n",
        "      X = X.view(batch_size, -1)\n",
        "      if layer_name not in features:\n",
        "        features[layer_name] = []\n",
        "      features[layer_name].append(X.detach().cpu())\n",
        "\n",
        "  labels = torch.cat(labels)\n",
        "  for layer_name in progress_bar(features.keys(), parent=mb):\n",
        "    features[layer_name] = torch.cat(features[layer_name], dim=0)\n",
        "    pairwise_similarity[layer_name] = np.corrcoef(features[layer_name])\n",
        "\n",
        "  model.stop_retaining_layers(layer_names)\n",
        "  model.close()\n",
        "\n",
        "  return features, labels, pairwise_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQI2UAUwOUYb"
      },
      "source": [
        "## AlexNet\n",
        "\n",
        "Let's explore the alexnet architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrV2tHP9MWgB"
      },
      "source": [
        "# pretrained=True allows loads weights for a model trained on ImageNet classification\n",
        "model = models.alexnet(pretrained=True)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1foTtdN4Ol9b"
      },
      "source": [
        "show_kernels(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RBwaysfQMRv"
      },
      "source": [
        "## Exercise 1 | Disentangled Object Representations\n",
        "\n",
        "- [ ] choose & download a dataset\n",
        "- [ ] create a dataloader\n",
        "- [ ] get activations from one layer\n",
        "- [ ] create an MDS plot to visualize whether your object categories are \"disentangled\" or \"entangled\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NifzqxrnRU3C"
      },
      "source": [
        "### download dataset\n",
        "\n",
        "Skip this step if you've already downloaded this dataset (click the folder icon on the left; if you see \"imagenette-320\" then you are all set).\n",
        "\n",
        "You can find a collection of vision datasets here:\n",
        "https://course.fast.ai/datasets\n",
        "\n",
        "We're going to download and \"unpack\" one of them, the \"imagenette\" subset, which is a subset of 10 image categories from the full ImageNet-1000 category dataset:\n",
        "\n",
        "https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SatajLyPYBV"
      },
      "source": [
        "!wget -c https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKl06nHR9Bn"
      },
      "source": [
        "!tar -xf imagenette-320.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6mmlQfSkQf"
      },
      "source": [
        "### setup dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN4njTTzVCdp"
      },
      "source": [
        "# for starters let's load our dataset without transforming to a \"tensor\" \n",
        "# it's just easier to visualize them this way\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize(224),\n",
        "  transforms.CenterCrop(224),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA8Hg_aBSATl"
      },
      "source": [
        "# first load the dataset without transforms so we can look at them\n",
        "dataset = datasets.ImageFolder('./imagenette-320/val', transform=transform)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcWeePnSS9Qw"
      },
      "source": [
        "show_images(dataset, num_categories=10, num_per_category=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS96dyT0UyMo"
      },
      "source": [
        "# ok, add the transforms needed to feed images to alexnet\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize(224),\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4rv0R4eWudt"
      },
      "source": [
        "# dataset with transforms\n",
        "dataset = datasets.ImageFolder('./imagenette-320/val', transform=transform)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXB1pqnwWzle"
      },
      "source": [
        "# dataloader for feeding batches to our model\n",
        "dataloader = DataLoader(dataset, batch_size=250, num_workers=8, pin_memory=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ys-vY-vXMz7"
      },
      "source": [
        "### get activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb9ZldVhXo0j"
      },
      "source": [
        "# features.0.12\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CK0bfBlXS-5"
      },
      "source": [
        "features, labels, pairwise_similarity = get_features(model, dataloader, layer_names=['features.12','classifier.6'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P465wZ9MdgaB"
      },
      "source": [
        "features['pixels'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHeY6O0diMj"
      },
      "source": [
        "features['features.12'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhLIMR5xea25"
      },
      "source": [
        "features['classifier.6'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWt8jZ4Hcxh4"
      },
      "source": [
        "import seaborn as sns \n",
        "from scipy.stats import pearsonr \n",
        "\n",
        "# take first two images, and scatter plot their features\n",
        "# we'll take the correlation as a measure of similarity\n",
        "img1_features = features['pixels'][0]\n",
        "img2_features = features['pixels'][1]\n",
        "corr = pearsonr(img1_features, img2_features)[0]\n",
        "print(corr)\n",
        "sns.scatterplot(x=img1_features, y=img2_features);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1sFxOpffrvV"
      },
      "source": [
        "# take first two images, and scatter plot their features\n",
        "# we'll take the correlation as a measure of similarity\n",
        "img1_features = features['classifier.6'][0]\n",
        "img2_features = features['classifier.6'][1]\n",
        "corr = pearsonr(img1_features, img2_features)[0]\n",
        "print(corr)\n",
        "sns.scatterplot(x=img1_features, y=img2_features);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pliyn8xLetrx"
      },
      "source": [
        "similarity_matrix = pairwise_similarity['classifier.6']\n",
        "mask = np.zeros_like(similarity_matrix)\n",
        "mask[np.tril_indices_from(mask)] = True\n",
        "ax = sns.heatmap(similarity_matrix, mask=mask, square=True, cmap=\"coolwarm\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBz7c4RPlFIe"
      },
      "source": [
        "mds_plot('classifier.6', pairwise_similarity, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9VeAiWAi4Mp"
      },
      "source": [
        "show_mds_plots(pairwise_similarity, labels, ncols=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oau5QqeUmofP"
      },
      "source": [
        "***Project Idea #1***\n",
        "It would be great if these points were clickable, so that we could click and \"see\" the images that were clustering next to each other, and click on \"oddballs\" and see if we can guess why they are clusting \"incorrectly\". Maybe the ones that don't fit in their cluster are just oddballs. To make this happen, you'd have to learn how to make interactive plots in \"jupyter notebooks\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkojACVhnHKW"
      },
      "source": [
        "# Exercise 2 | Analyze a new dataset, or new model, or both!\n",
        "\n",
        "***1) try a different dataset***   \n",
        "You can find other easy-to-work-with datasets here: https://course.fast.ai/datasets\n",
        "\n",
        "After you see the options there, you can develop a hypothesis. e.g., Are sub-categories more entangled (compare imagenette with imagewoof, or the \"flowers\" dataset).\n",
        "\n",
        "***related project idea#2*** \n",
        "The machine vision community has generated tons of image and video datasets, and you might generate some testable ideas looking at those. There are many lists/resources, but this might be a good place to start https://www.kaggle.com/datasets\n",
        "\n",
        "One interesting question related to this week's reading is whether a CNN trained on regular images would struggle with line-drawings (try googling \"computer vision line drawings dataset\"). \n",
        "\n",
        "***2) try a different model***\n",
        "The torchvision package has different models: https://pytorch.org/vision/0.8/models.html\n",
        "\n",
        "Many of these models were trained on ImageNet-1000 classification, but others were trained on semantic segmentation or object detection. The easiest thing would be to grab a new object-classification network first, since networks trained for other tasks might have unfamiliar architectures  making it harder to work with them (but you have me here to help!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqRg-bfmmngf"
      },
      "source": [
        "# notice that this model has a transform built in! \n",
        "# So we'll want to re-create our dataset without Resize, Crop, or Normalize transforms\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkc24UtwrM5l"
      },
      "source": [
        "# model.rpn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rTIFig1iMYf"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "  transforms.Resize(320),\n",
        "  transforms.CenterCrop(320),\n",
        "  transforms.ToTensor(),\n",
        "  #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder('./imagenette-320/val', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=50, num_workers=8, pin_memory=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-usRqJrp9zr"
      },
      "source": [
        "features, labels, pairwise_similarity = get_features(model, dataloader, layer_names=['backbone.body.layer4','rpn'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNormXgAr1SC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}