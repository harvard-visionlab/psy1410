{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "psy14010_deep_reinforcement_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP971OvrQlwRz2FUK18wqyK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/psy1410/blob/master/psy14010_deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryzwzEILPza-"
      },
      "source": [
        "# https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Q_J2oaPund",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "c0e8aca6-025a-46b5-b396-5e2142147787"
      },
      "source": [
        "!pip install gym-super-mario-bros==7.3.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-super-mario-bros==7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b8/07460212c2568f78b02995834e7bdc25349e586473919e2983e01b984abf/gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 6.7MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/5e/d652644d718454947b9e26d8a145eb7d1eff0f014dceee7bd88e4894b3f3/nes_py-8.1.6.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Collecting tqdm>=4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.6-cp37-cp37m-linux_x86_64.whl size=436569 sha256=52cf47d4e6b6e0df14e58d72c98b746095811d25f9653635328383db48334e8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/56/af/b84114d31ea6301a5c4651fb048bd6072646596a6ceb3bbc24\n",
            "Successfully built nes-py\n",
            "Installing collected packages: tqdm, nes-py, gym-super-mario-bros\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.6 tqdm-4.60.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97fNb3YXP4HX"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvfYNK-YP9QT",
        "outputId": "32a20f62-8cc1-4833-8797-b361bf81e8e8"
      },
      "source": [
        "# Initialize Super Mario environment\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozxr88rtP-5u"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "iT5tLF1xQCm6",
        "outputId": "d0346cbf-9fa5-4aa7-8a2e-8a882807b75a"
      },
      "source": [
        "from PIL import Image \n",
        "screen = env.render(mode='rgb_array')\n",
        "Image.fromarray(screen)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x240 at 0x7F928D3B8350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU38t8x1QGG8"
      },
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"Add the experience to memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"Sample experiences from memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "        pass"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSgOT6IiQb6V"
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir, online_net):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim, save_dir, online_net).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    action_idx (int): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHP0B564QetH"
      },
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir, online_net):\n",
        "        super().__init__(state_dim, action_dim, save_dir, online_net)\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda()\n",
        "        else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci7ElgoUaYS3"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-pTSF5qaZNA"
      },
      "source": [
        "## Jordy, to test different backbones you would change MarioNet down below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8jIPRIIavsu",
        "outputId": "8ba66160-c6a6-43b7-b905-90aa16a0520f"
      },
      "source": [
        "# install a repository of pre-trained image models, where we'll get a vision transformer\n",
        "!pip install git+git://github.com/rwightman/pytorch-image-models.git -U --user"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/rwightman/pytorch-image-models.git\n",
            "  Cloning git://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-fr5503ig\n",
            "  Running command git clone -q git://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-fr5503ig\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.8) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.8) (0.9.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.8) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.8) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.8) (7.1.2)\n",
            "Building wheels for collected packages: timm\n",
            "  Building wheel for timm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timm: filename=timm-0.4.8-cp37-none-any.whl size=338237 sha256=8443b1d4cca44ba174fab48f1cf4883c5c3506ef10cd98ad947613e41001ee00\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bmumfe5a/wheels/dd/06/42/b6dde8d04ae897c24a48d3fc292f938e2db46aa75023f8e2ec\n",
            "Successfully built timm\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRGHp-bhWjRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ef1e69-a9ed-4075-c74a-21cb1030dd8f"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eJA_yW1a_kn",
        "outputId": "f75a49df-1337-4c05-80eb-ea53dca6d7be"
      },
      "source": [
        "# get alexnet from torchvision\n",
        "import torchvision \n",
        "\n",
        "# notice that we're setting the output num_classes to the number of possible actions\n",
        "# so each output node corresponds to an action\n",
        "alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "# but then we need to \"remove the imagenet layer\" (last fully connected layer)\n",
        "# and then we replace it with a fully connected layer with number of outputs\n",
        "# equal to the number of possible actions\n",
        "in_features = alexnet.classifier[6].in_features\n",
        "alexnet.classifier[6] = nn.Linear(in_features, env.action_space.n)\n",
        "\n",
        "alexnet"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddjvdvked1Ul"
      },
      "source": [
        "# We can also get a vision transformer from the timm module\n",
        "# https://rwightman.github.io/pytorch-image-models/\n",
        "import timm\n",
        "from pprint import pprint\n",
        "import torch.nn as nn\n",
        "model_names = timm.list_models('*vit*')\n",
        "# pprint(model_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z1KAQKDeEgB",
        "outputId": "ce038f1c-7dfe-4cee-de62-9f9d773fc535"
      },
      "source": [
        "transformer = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "in_features = transformer.head.in_features\n",
        "transformer.head = nn.Linear(in_features, env.action_space.n)\n",
        "transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (pre_logits): Identity()\n",
              "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXKT4eLRXjqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc15183-201e-45fe-8df1-8f2458973869"
      },
      "source": [
        "transformer = torch.hub.load('facebookresearch/dino:main', 'dino_deits8')\n",
        "transformer.head = nn.Linear(384, env.action_space.n)\n",
        "transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(8, 8), stride=(8, 8))\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): ModuleList(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Linear(in_features=384, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnhKRbUKQg6e"
      },
      "source": [
        "from IPython.core.debugger import set_trace \n",
        "\n",
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, save_dir, online_net):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        # let's replace the demo's CNN with our own, passed in as online_net\n",
        "        # self.online = nn.Sequential(\n",
        "        #     nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Flatten(),\n",
        "        #     nn.Linear(3136, 512),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(512, output_dim),\n",
        "        # )\n",
        "\n",
        "        self.online = online_net\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2h18O9QkjD"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, online_net):\n",
        "        super().__init__(state_dim, action_dim, save_dir, online_net)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbqLjNzeQnSt"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, online_net):\n",
        "        super().__init__(state_dim, action_dim, save_dir, online_net)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk2mVh-yQpW9"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaw49UXBQruH"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, online_net):\n",
        "        super().__init__(state_dim, action_dim, save_dir, online_net)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPMYsRIRQt8R"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpdc9_1fYF1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8053aa77-3a1b-47be-fcc3-dff2d96b76b3"
      },
      "source": [
        "output_dim = env.action_space.n\n",
        "online_net = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(3136, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, output_dim),\n",
        ")\n",
        "online_net"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "  (1): ReLU()\n",
              "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (3): ReLU()\n",
              "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (5): ReLU()\n",
              "  (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  (7): Linear(in_features=3136, out_features=512, bias=True)\n",
              "  (8): ReLU()\n",
              "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjO_sGaWaDI9",
        "outputId": "0c38d42b-448a-4a09-b8f2-84c3e2d5f841"
      },
      "source": [
        "# notice that we're setting the output num_classes to the number of possible actions\n",
        "# so each output node corresponds to an action\n",
        "online_net = torchvision.models.alexnet(pretrained=True)\n",
        "# but then we need to \"remove the imagenet layer\" (last fully connected layer)\n",
        "# and then we replace it with a fully connected layer with number of outputs\n",
        "# equal to the number of possible actions\n",
        "in_features = alexnet.classifier[6].in_features\n",
        "online_net.classifier[6] = nn.Linear(in_features, env.action_space.n)\n",
        "online_net.features[0] = nn.Conv2d(4, 64, (11, 11), (4, 4), (2, 2))\n",
        "\n",
        "online_net"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "PbnvXgLIQwzh",
        "outputId": "2663cf80-7e8d-43c3-ec0e-b2d1b54c641b"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, \n",
        "              online_net=online_net)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 10\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0 - Step 98 - Epsilon 0.9999755002970567 - Mean Reward 626.0 - Mean Length 98.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.112 - Time 2021-05-03T18:10:09\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P36EI7_YQ3Lm"
      },
      "source": [
        "# env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCd3YVOVQ570"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "HZ4pb4veXrXV",
        "outputId": "5f6a8aa8-5a4f-4d8c-833c-680fe78f5ccb"
      },
      "source": [
        "from IPython import display as ipythondisplay\n",
        "\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  #action = env.action_space.sample()\n",
        "  action = mario.act(state)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Alm2-8X2px"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}