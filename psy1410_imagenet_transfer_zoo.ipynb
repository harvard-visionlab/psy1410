{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "psy1410_imagenet_transfer_zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1miOF9ubV_V5",
        "BRI6rg1BOkR_",
        "BF52WI-2_So9",
        "EzbHJM3bTxG1",
        "xhXGr8vZT4Kj"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2GWIz2REzep6NerS8AzKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc4d9c85c61948d294f2dde7b57ca2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e31218627274f96b6bba674b98dae42",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c6a3f0793f44aa0b6f6b3db4e9eaccb",
              "IPY_MODEL_fa6308d0b98e4a9a8213eb77efbca347"
            ]
          }
        },
        "1e31218627274f96b6bba674b98dae42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c6a3f0793f44aa0b6f6b3db4e9eaccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_509998d040ac4fec85f4e3627b67f200",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 110696066,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 110696066,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5638554e067f40a7b704975dd392cc98"
          }
        },
        "fa6308d0b98e4a9a8213eb77efbca347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ef2ebb7cb014280b3352fd8360c00cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 106M/106M [01:07&lt;00:00, 1.65MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bf3d438fd254e3f8cf751df08cf7987"
          }
        },
        "509998d040ac4fec85f4e3627b67f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5638554e067f40a7b704975dd392cc98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ef2ebb7cb014280b3352fd8360c00cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bf3d438fd254e3f8cf751df08cf7987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e327c6171114e7abf77909a106d314c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd8e8a2055d14dc78719a4965dfc75cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_07b5eabfb14046f997cd400c9bce58c7",
              "IPY_MODEL_310c2da920f94d70ac67cdf949c35368"
            ]
          }
        },
        "bd8e8a2055d14dc78719a4965dfc75cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07b5eabfb14046f997cd400c9bce58c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ee4842d02d68433aa2547d6db1473a42",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 115887415,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 115887415,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_917561666a7948d0aaff539355a91005"
          }
        },
        "310c2da920f94d70ac67cdf949c35368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7532cf9c07d24f43b5ec73b0b718574d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 111M/111M [00:01&lt;00:00, 66.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1226b8abebc4a018fb511da99c94611"
          }
        },
        "ee4842d02d68433aa2547d6db1473a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "917561666a7948d0aaff539355a91005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7532cf9c07d24f43b5ec73b0b718574d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1226b8abebc4a018fb511da99c94611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "181292c8cfd14af3bbc4d257d0f395fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_177f90b3fc9b4263bf06d4ea35d7c030",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fb55a2f6ff04ecd90b76033e4fafeeb",
              "IPY_MODEL_ae84f52dfae6429a8c91eed635a403f4"
            ]
          }
        },
        "177f90b3fc9b4263bf06d4ea35d7c030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fb55a2f6ff04ecd90b76033e4fafeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34ba4e9eac15473f9f6fcf9a4bc03c38",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 126580806,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 126580806,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09a7d97e9c7d4664ad86ca364eb6ea38"
          }
        },
        "ae84f52dfae6429a8c91eed635a403f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fbd0ca6f54e4d3cbd18cc0d10df3085",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 121M/121M [00:01&lt;00:00, 81.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4caa70d4ef8d4fd4a5c30c5511020ab5"
          }
        },
        "34ba4e9eac15473f9f6fcf9a4bc03c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09a7d97e9c7d4664ad86ca364eb6ea38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fbd0ca6f54e4d3cbd18cc0d10df3085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4caa70d4ef8d4fd4a5c30c5511020ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/psy1410/blob/master/psy1410_imagenet_transfer_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xnNvzNC-3b8"
      },
      "source": [
        "# Imagenet Transfer Model Zoo\n",
        "\n",
        "This notebook is our storehouse of models that have been trained on one task (e.g., video action recognition, or face recognition), followed by transfer training to test performance on imagenet classification. For this reason, the original \"task head\" has been removed, replaced with a fresh fully-connected layer with 1000 output units (corresponding to the Imagenet categories). The convolutional-backbone's weights were frozen, and only the new 1000 unit fully-connected layer had its weights adjusted. So we're asking \"how well can we use the features of this model, trained on X, to perform imagenet classification?\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1cLWKFS-zaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1miOF9ubV_V5"
      },
      "source": [
        "# installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2_TLx67WCXT",
        "outputId": "919e7360-0377-4270-e363-9635db9e5475"
      },
      "source": [
        "!pip install facenet-pytorch"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting facenet-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e8/5ea742737665ba9396a8a2be3d2e2b49a13804b56a7e7bb101e8731ade8f/facenet_pytorch-2.5.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet-pytorch) (3.7.4.3)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRI6rg1BOkR_"
      },
      "source": [
        "# helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CumSdsjOlsO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "def show_conv1(model):\n",
        "    for m in [module for module in model.modules() if type(module) != nn.Sequential]:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            break\n",
        "    kernels = m.weight.detach().clone().cpu()\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    img = make_grid(kernels, nrow=16)\n",
        "    ax = plt.imshow(img.permute(1, 2, 0))\n",
        "    return ax"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sL3Nyh_RDU"
      },
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF52WI-2_So9"
      },
      "source": [
        "### moments action-recognition models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KassO5X_QJG"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import functools\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3x3 convolution with padding.\"\"\"\n",
        "    return nn.Conv3d(\n",
        "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "def downsample_basic_block(x, planes, stride):\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    zero_pads = torch.Tensor(\n",
        "        out.size(0), planes - out.size(1),\n",
        "        out.size(2), out.size(3), out.size(4)).zero_()\n",
        "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "        zero_pads = zero_pads.cuda()\n",
        "    out = torch.cat([out.data, zero_pads], dim=1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    Conv3d = staticmethod(conv3x3x3)\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = self.Conv3d(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = self.Conv3d(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    Conv3d = nn.Conv3d\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = self.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = self.Conv3d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = self.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet3D(nn.Module):\n",
        "\n",
        "    Conv3d = nn.Conv3d\n",
        "\n",
        "    def __init__(self, block, layers, shortcut_type='B', num_classes=305):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet3D, self).__init__()\n",
        "        self.conv1 = self.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(\n",
        "                    downsample_basic_block,\n",
        "                    planes=planes * block.expansion,\n",
        "                    stride=stride,\n",
        "                )\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    self.Conv3d(\n",
        "                        self.inplanes,\n",
        "                        planes * block.expansion,\n",
        "                        kernel_size=1,\n",
        "                        stride=stride,\n",
        "                        bias=False,\n",
        "                    ),\n",
        "                    nn.BatchNorm3d(planes * block.expansion),\n",
        "                )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, self.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def modify_resnets(model):\n",
        "    # Modify attributs\n",
        "    model.last_linear, model.fc = model.fc, None\n",
        "\n",
        "    def features(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "    def logits(self, features):\n",
        "        x = self.avgpool(features)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.last_linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.features(input)\n",
        "        x = self.logits(x)\n",
        "        return x\n",
        "\n",
        "    # Modify methods\n",
        "    setattr(model.__class__, 'features', features)\n",
        "    setattr(model.__class__, 'logits', logits)\n",
        "    setattr(model.__class__, 'forward', forward)\n",
        "    return model\n",
        "\n",
        "\n",
        "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
        "weights = {\n",
        "    'resnet50': 'moments_v2_RGB_resnet50_imagenetpretrained.pth.tar',\n",
        "    'resnet3d50': 'moments_v2_RGB_imagenet_resnet3d50_segment16.pth.tar',\n",
        "    'multi_resnet3d50': 'multi_moments_v2_RGB_imagenet_resnet3d50_segment16.pth.tar',\n",
        "}\n",
        "default_model_dir = '/content/weights'\n",
        "if not os.path.exists(default_model_dir):\n",
        "    os.makedirs(default_model_dir)\n",
        "\n",
        "def load_checkpoint(weight_file):\n",
        "    weight_file_name = weight_file\n",
        "    weight_file = os.path.join(default_model_dir, weight_file)\n",
        "    if not os.access(weight_file, os.W_OK):\n",
        "        weight_url = os.path.join(ROOT_URL, weight_file_name)\n",
        "        os.system('wget ' + weight_url)\n",
        "    checkpoint = torch.load(weight_file, map_location=lambda storage, loc: storage)  # Load on cpu\n",
        "    return {str.replace(str(k), 'module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "\n",
        "\n",
        "def resnet50(num_classes=305, pretrained=True):\n",
        "    model = models.__dict__['resnet50'](num_classes=num_classes)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(load_checkpoint(weights['resnet50']))\n",
        "    model = modify_resnets(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet3d50(num_classes=305, pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet3D-50 model.\"\"\"\n",
        "    model = modify_resnets(ResNet3D(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs))\n",
        "    if pretrained:\n",
        "         model.load_state_dict(load_checkpoint(weights['resnet3d50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def multi_resnet3d50(num_classes=292, pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet3D-50 model.\"\"\"\n",
        "    model = modify_resnets(ResNet3D(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs))\n",
        "    if pretrained:\n",
        "        model.load_state_dict(load_checkpoint(weights['multi_resnet3d50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_model(arch):\n",
        "    model = {'resnet3d50': resnet3d50,\n",
        "             'multi_resnet3d50': multi_resnet3d50, 'resnet50': resnet50}.get(arch, 'resnet3d50')()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_transform():\n",
        "    \"\"\"Load the image transformer.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "def load_categories(filename):\n",
        "    \"\"\"Load categories.\"\"\"\n",
        "    with open(filename) as f:\n",
        "        return [line.rstrip() for line in f.readlines()]\n",
        "\n",
        "def load_frames(frame_paths, num_frames=8):\n",
        "    \"\"\"Load PIL images from a list of file paths.\"\"\"\n",
        "    frames = [Image.open(frame).convert('RGB') for frame in frame_paths]\n",
        "    if len(frames) >= num_frames:\n",
        "        return frames[::int(np.ceil(len(frames) / float(num_frames)))]\n",
        "    else:\n",
        "        raise ValueError('Video must have at least {} frames'.format(num_frames))\n",
        "        \n",
        "def extract_frames(video_file, num_frames=16):\n",
        "    \"\"\"Return a list of PIL image frames uniformly sampled from an mp4 video.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.join(os.getcwd(), 'frames'))\n",
        "    except OSError:\n",
        "        pass\n",
        "    output = subprocess.Popen(['ffmpeg', '-i', video_file],\n",
        "                              stderr=subprocess.PIPE).communicate()\n",
        "    # Search and parse 'Duration: 00:05:24.13,' from ffmpeg stderr.\n",
        "    re_duration = re.compile(r'Duration: (.*?)\\.')\n",
        "    duration = re_duration.search(str(output[1])).groups()[0]\n",
        "\n",
        "    seconds = functools.reduce(lambda x, y: x * 60 + y,\n",
        "                               map(int, duration.split(':')))\n",
        "    rate = num_frames / float(seconds)\n",
        "\n",
        "    output = subprocess.Popen(['ffmpeg', '-i', video_file,\n",
        "                               '-vf', 'fps={}'.format(rate),\n",
        "                               '-vframes', str(num_frames),\n",
        "                               '-loglevel', 'panic',\n",
        "                               'frames/%d.jpg']).communicate()\n",
        "    frame_paths = sorted([os.path.join('frames', frame)\n",
        "                          for frame in os.listdir('frames')])\n",
        "    frames = load_frames(frame_paths, num_frames=num_frames)\n",
        "    subprocess.call(['rm', '-rf', 'frames'])\n",
        "    return frames"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaU8UR4NJw57"
      },
      "source": [
        "from torchvision import transforms, models  \n",
        "from torch.hub import load_state_dict_from_url\n",
        "from pathlib import Path \n",
        "from IPython.core.debugger import set_trace \n",
        "\n",
        "VISLAB_URL = \"https://visionlab-pretrainedmodels.s3.amazonaws.com/model_zoo/psy1410/\"\n",
        "default_model_dir = '/content/weights'\n",
        "if not os.path.exists(default_model_dir):\n",
        "  os.makedirs(default_model_dir)\n",
        "\n",
        "def load_checkpoint_imagenet_head(model, weights_url, weight_dir=None, device='cpu'):\n",
        "  model_dir = default_model_dir if weight_dir is None else weight_dir\n",
        "\n",
        "  weights_url = str(weights_url)\n",
        "\n",
        "  print(f\"=> loading checkpoint: {Path(weights_url).name}\")\n",
        "  checkpoint = load_state_dict_from_url(weights_url, model_dir=model_dir, \n",
        "                                        map_location=torch.device(device))\n",
        "  state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "  model.load_state_dict(state_dict)\n",
        "  print(\"=> state loaded.\")\n",
        "\n",
        "  model.top1 = checkpoint['top1']\n",
        "  model.num_epochs = checkpoint['epoch']\n",
        "  print(f\"=> top1 accuracy {model.top1:3.2f}% (num_epochs={model.num_epochs})\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def moments_resnet3d50_imagenet_head():\n",
        "  model = resnet3d50(num_classes=1000, pretrained=False)\n",
        "  weights_url = VISLAB_URL+'moments_resnet3d50_avgpool_onecycle.pth.tar'\n",
        "  model = load_checkpoint_imagenet_head(model, weights_url)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "  \n",
        "  test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "  ])\n",
        "\n",
        "  return model, test_transforms\n",
        "\n",
        "def moments_resnet50_imagenet_head():\n",
        "  model = resnet50(num_classes=1000, pretrained=False)\n",
        "\n",
        "  weights_url = VISLAB_URL+'moments_resnet50_avgpool_onecycle.pth.tar'\n",
        "  model = load_checkpoint_imagenet_head(model, weights_url)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "  \n",
        "  test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "  ])\n",
        "\n",
        "  return model, test_transforms\n",
        "\n",
        "def imagenet_resnet50_imagenet_head():\n",
        "  model = models.resnet50(num_classes=1000, pretrained=False)  \n",
        "  weights_url = VISLAB_URL+'imagenet_resnet50_avgpool_onecycle.pth.tar'\n",
        "  model = load_checkpoint_imagenet_head(model, weights_url)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "  \n",
        "  test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "  ])\n",
        "\n",
        "  return model, test_transforms\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tfQBv9_XkeG"
      },
      "source": [
        "### FaceNet face-recognition models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IXIBZfUXq4F"
      },
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from facenet_pytorch import fixed_image_standardization\n",
        "\n",
        "from torchvision import transforms, models  \n",
        "from torch.hub import load_state_dict_from_url\n",
        "from pathlib import Path \n",
        "from IPython.core.debugger import set_trace \n",
        "\n",
        "VISLAB_URL = \"https://visionlab-pretrainedmodels.s3.amazonaws.com/model_zoo/psy1410/\"\n",
        "default_model_dir = '/content/weights'\n",
        "if not os.path.exists(default_model_dir):\n",
        "  os.makedirs(default_model_dir)\n",
        "\n",
        "def load_checkpoint_imagenet_head(model, weights_url, weight_dir=None, device='cpu'):\n",
        "  model_dir = default_model_dir if weight_dir is None else weight_dir\n",
        "\n",
        "  weights_url = str(weights_url)\n",
        "\n",
        "  print(f\"=> loading checkpoint: {Path(weights_url).name}\")\n",
        "  checkpoint = load_state_dict_from_url(weights_url, model_dir=model_dir, \n",
        "                                        map_location=torch.device(device))\n",
        "  state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "  model.load_state_dict(state_dict)\n",
        "  print(\"=> state loaded.\")\n",
        "\n",
        "  model.top1 = checkpoint['top1']\n",
        "  model.num_epochs = checkpoint['epoch']\n",
        "  print(f\"=> top1 accuracy {model.top1:3.2f}% (num_epochs={model.num_epochs})\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def vggface2_inceptionV1_imagenet_head():\n",
        "  model = InceptionResnetV1(pretrained='vggface2')\n",
        "  in_features = model.last_linear.in_features\n",
        "  model.last_linear = nn.Linear(in_features, 1000)\n",
        "  model.last_linear.weight.data.normal_(mean=0.0, std=0.01)\n",
        "  model.last_linear.bias.data.zero_()\n",
        "  model.last_bn = nn.BatchNorm1d(1000, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  \n",
        "  weights_url = VISLAB_URL+'vggface2_inceptionV1_avgpool_1a_onecycle.pth.tar'\n",
        "  model = load_checkpoint_imagenet_head(model, weights_url)\n",
        "\n",
        "  test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    fixed_image_standardization\n",
        "  ])\n",
        "\n",
        "  return model, test_transforms \n",
        "\n",
        "def casia_inceptionV1_imagenet_head():\n",
        "  model = InceptionResnetV1(pretrained='casia-webface')\n",
        "  in_features = model.last_linear.in_features\n",
        "  model.last_linear = nn.Linear(in_features, 1000)\n",
        "  model.last_linear.weight.data.normal_(mean=0.0, std=0.01)\n",
        "  model.last_linear.bias.data.zero_()\n",
        "  model.last_bn = nn.BatchNorm1d(1000, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "  weights_url = VISLAB_URL+'casia_inceptionV1_avgpool_1a_onecycle.pth.tar'\n",
        "  model = load_checkpoint_imagenet_head(model, weights_url)\n",
        "\n",
        "  test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    fixed_image_standardization\n",
        "  ])\n",
        "\n",
        "  return model, test_transforms \n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzbHJM3bTxG1"
      },
      "source": [
        "# Loading Moments-trained Models (Annie)\n",
        "\n",
        "These models were pre-trained on: (1) action recognition from video (moments_resnet3d50), (2) action recognition from still images (moments_resnet50), or (3) imagenet recognition (imagenet_resnet50).\n",
        "\n",
        "For each network, I removed the fully-connected layer and replaced it with one that has 1000 output units, for the 1000 ImageNet classes.\n",
        "\n",
        "All of the weights are frozen, except those of this last fully-connected layer, which I trained on ImageNet classification.\n",
        "\n",
        "So this new FC layer is used to \"readout a fixed set of features from the convolutional backbone.\"\n",
        "\n",
        "One tricky part was that resnet3d50 expects sequences of 16 video frames as it's input. What I did was repeat the same image 16 times, which we can think of as a \"very slow moving video\"! I couldn't really think of a better way to train this model to do imagenet recognition! As you'll see it doesn't perform as well as the model trained on still images (either on moments or imagenet), but it still does well above chances (34.7% vs. chance which is 1/1000=.10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0OZXb1Mrod",
        "outputId": "8054dca7-946b-4397-ff4a-3c4945b7a32b"
      },
      "source": [
        "# model trained with videos (Moments \"action recognition\")\n",
        "# remove it's fully-connected layer, replace with a new one that\n",
        "# has 1000 outputs (corresponding to the 1000 imagenet categories)\n",
        "# train this new layer on ImageNet Classification.\n",
        "model, test_transforms = moments_resnet3d50_imagenet_head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint: moments_resnet3d50_avgpool_onecycle.pth.tar\n",
            "=> state loaded.\n",
            "=> top1 accuracy 34.72% (num_epochs=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155,
          "referenced_widgets": [
            "fc4d9c85c61948d294f2dde7b57ca2ad",
            "1e31218627274f96b6bba674b98dae42",
            "6c6a3f0793f44aa0b6f6b3db4e9eaccb",
            "fa6308d0b98e4a9a8213eb77efbca347",
            "509998d040ac4fec85f4e3627b67f200",
            "5638554e067f40a7b704975dd392cc98",
            "9ef2ebb7cb014280b3352fd8360c00cf",
            "0bf3d438fd254e3f8cf751df08cf7987"
          ]
        },
        "id": "g_1Ps0FxPsXv",
        "outputId": "c90946d0-5201-4dd6-c481-4634acdbf5b1"
      },
      "source": [
        "# model trained with static images on Moments \"action recognition\"\n",
        "# remove it's fully-connected layer, replace with a new one that\n",
        "# has 1000 outputs (corresponding to the 1000 imagenet categories)\n",
        "# train this new layer on ImageNet Classification.\n",
        "model, test_transforms = moments_resnet50_imagenet_head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint: moments_resnet50_avgpool_onecycle.pth.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://visionlab-pretrainedmodels.s3.amazonaws.com/model_zoo/psy1410/moments_resnet50_avgpool_onecycle.pth.tar\" to /content/weights/moments_resnet50_avgpool_onecycle.pth.tar\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc4d9c85c61948d294f2dde7b57ca2ad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=110696066.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=> state loaded.\n",
            "=> top1 accuracy 44.00% (num_epochs=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy5ZWNhVO7Au",
        "outputId": "c45adca1-0b9d-468b-e419-99af157ff691"
      },
      "source": [
        "# This is a sanity check to make sure the ImageNet training is reasonable.\n",
        "# Here we take a network trained on ImageNet classification to begin with,\n",
        "# then cut off it's fully-connected layer, and replace it with an untrained\n",
        "# one. That new layer is then trained on ImageNet Classification. If the\n",
        "# procedure is reasonable we should get close to the original performance\n",
        "# (which we do here, 73% new, 76% original).\n",
        "model, test_transforms = imagenet_resnet50_imagenet_head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint: imagenet_resnet50_avgpool_onecycle.pth.tar\n",
            "=> state loaded.\n",
            "=> top1 accuracy 72.95% (num_epochs=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhXGr8vZT4Kj"
      },
      "source": [
        "# Loading FaceNet Models (Jolade?)\n",
        "\n",
        "These models were pre-trained on the FaceNet triplet task.\n",
        "\n",
        "Then I removed the fully-connected layer and replaced it with one that has 1000 output units, for the 1000 ImageNet classes.\n",
        "\n",
        "All of the weights are frozen, except those of this last fully-connected layer, which I trained on ImageNet classification.\n",
        "\n",
        "So this new FC layer is used to \"readout a fixed set of features from the convolutional backbone specialized for face processing.\"\n",
        "\n",
        "There are two different FaceNet models, trained on different face datasets (the vggface2, and casia-web datasets).\n",
        "\n",
        "One issue we might have to address is that the overall performance level for these networks on imagenet classificaiton is low. Perhaps that's to be expected: These are highly face-specialized networks, and they might not have \"the right kinds of features\" for performing imagenet classification.\n",
        "\n",
        "That said, even though performnce is low (14% and 18%), they are far above chance (which is 1/1000, or .1%)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2fAZSi9OXI8",
        "outputId": "772a2a13-6c5d-40f3-855d-a33393a3a0ab"
      },
      "source": [
        "model, test_transform = vggface2_inceptionV1_imagenet_head()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint: vggface2_inceptionV1_avgpool_1a_onecycle.pth.tar\n",
            "=> state loaded.\n",
            "=> top1 accuracy 14.25% (num_epochs=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "1e327c6171114e7abf77909a106d314c",
            "bd8e8a2055d14dc78719a4965dfc75cd",
            "07b5eabfb14046f997cd400c9bce58c7",
            "310c2da920f94d70ac67cdf949c35368",
            "ee4842d02d68433aa2547d6db1473a42",
            "917561666a7948d0aaff539355a91005",
            "7532cf9c07d24f43b5ec73b0b718574d",
            "f1226b8abebc4a018fb511da99c94611",
            "181292c8cfd14af3bbc4d257d0f395fb",
            "177f90b3fc9b4263bf06d4ea35d7c030",
            "0fb55a2f6ff04ecd90b76033e4fafeeb",
            "ae84f52dfae6429a8c91eed635a403f4",
            "34ba4e9eac15473f9f6fcf9a4bc03c38",
            "09a7d97e9c7d4664ad86ca364eb6ea38",
            "1fbd0ca6f54e4d3cbd18cc0d10df3085",
            "4caa70d4ef8d4fd4a5c30c5511020ab5"
          ]
        },
        "id": "4h_4-KHFOXLq",
        "outputId": "93b65f48-a48c-458b-e17d-203b15a60623"
      },
      "source": [
        "model, test_transform = casia_inceptionV1_imagenet_head()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e327c6171114e7abf77909a106d314c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=115887415.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=> loading checkpoint: casia_inceptionV1_avgpool_1a_onecycle.pth.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://visionlab-pretrainedmodels.s3.amazonaws.com/model_zoo/psy1410/casia_inceptionV1_avgpool_1a_onecycle.pth.tar\" to /content/weights/casia_inceptionV1_avgpool_1a_onecycle.pth.tar\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "181292c8cfd14af3bbc4d257d0f395fb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=126580806.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=> state loaded.\n",
            "=> top1 accuracy 17.80% (num_epochs=10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYiAIaricDM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}