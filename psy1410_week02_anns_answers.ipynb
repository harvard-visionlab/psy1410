{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "psy1410_week02_anns_answers.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPecAU5z7RbyF9KHYSAPxDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/psy1410/blob/master/psy1410_week02_anns_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM_hoFmZeVXx"
      },
      "source": [
        "## Helpers\n",
        "\n",
        "Here we'll define any helper functions that we need/want as we go. We'll probably add to this as we find a need for new helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUoTnJyslo4V"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPfYSf69eUVp"
      },
      "source": [
        "import numpy as np \n",
        "from PIL import Image \n",
        "from IPython.core.debugger import set_trace \n",
        "\n",
        "def show_image(img):\n",
        "  return Image.fromarray( (img * 256).squeeze().numpy().astype(np.uint8) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUECmnLaa0HS"
      },
      "source": [
        "## A Minimal ANN\n",
        "\n",
        "Let's start by defining a very minimal artificial neural network, with a single fully-connected linear layer that directly maps the input (1x28x28 pixels) to the output categories (10 digit categories)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgYzIL2La-5G"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWf6zhV0aVhh"
      },
      "source": [
        "class MyNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyNet, self).__init__()\n",
        "    # in_features = 784, because the input image is 1x28x28 = 784\n",
        "    # out_features = 10, because there are 10 output categories (digits 0-9)\n",
        "    self.fc = nn.Linear(in_features=784, out_features=10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # in the \"forward pass\", we take an input (a batch of images, x)\n",
        "    # then first we flatten it into batchSize x 784, \n",
        "    batchSize = x.shape[0] # first dimension of x is \"batchSize\"\n",
        "    x = x.view(batchSize, -1) # the -1 tells pytorch to flatten the tensor to be batchSize x \"whatever size fits\"\n",
        "\n",
        "    # finally, we pass the flattened input into our fully-connected layer \n",
        "    # which will compute the weighted sum of the input for each of the 10 \n",
        "    # categories\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVF5WTYma8kZ"
      },
      "source": [
        "# create an instance of MyNet\n",
        "model = MyNet()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV5uyUnygtmy"
      },
      "source": [
        "# test on random data (100 random images)\n",
        "fake_imgs = torch.rand(100,1,28,28)\n",
        "out = model(fake_imgs)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5fS7spwhCAH"
      },
      "source": [
        "# why is the output shape \"100x10\"?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2_dgHWIhfcj"
      },
      "source": [
        "## Inspect/visualize the weights of your randomly intialized network\n",
        "\n",
        "Let's write a function that takes the weights of our model and visualizes them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wU3Lt7Vhboa"
      },
      "source": [
        "model.fc.weight.shape, model.fc.bias.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqkdbYJ_h7l6"
      },
      "source": [
        "w = model.fc.weight[0].detach().reshape(28,28)\n",
        "w.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JbQ1tvoihQ3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(w, extent=[0, 1, 0, 1], cmap='coolwarm');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLlTrscujKyW"
      },
      "source": [
        "def show_weights(model):\n",
        "  idx = -1\n",
        "  fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
        "  for row in axs:\n",
        "    for ax in row:\n",
        "      idx += 1\n",
        "      w = model.fc.weight[idx].detach().reshape(28,28)\n",
        "      ax.imshow(w, extent=[0, 1, 0, 1], cmap='coolwarm')\n",
        "      ax.set_title(f\"label={idx}\")\n",
        "      ax.grid(True)\n",
        "      ax.axes.get_xaxis().set_visible(False)\n",
        "      ax.axes.get_yaxis().set_visible(False)\n",
        "  plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Jp6ZCimJ55"
      },
      "source": [
        "show_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQPzpWLxnmWo"
      },
      "source": [
        "## Let's Train this Model!\n",
        "\n",
        "We'll need:\n",
        "- [x] a model\n",
        "- [ ] a dataset (MNIST)\n",
        "- [ ] a loss function (Cross Entropy Loss)\n",
        "- [ ] an optimizer (which will do all of the `back-propogation of errors` that we need to modify the weights\n",
        "- [ ] we need a training function\n",
        "- [ ] useful to have a validation function too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z78ZTwlUbv1b"
      },
      "source": [
        "## MNIST Dataset\n",
        "\n",
        "- we'll start with the standard MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O9s7ZyubibD"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms \n",
        "\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQBx0wob0fg"
      },
      "source": [
        "train_dataset = datasets.MNIST('./data/MNIST', train=True, download=True, transform=transform)\n",
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTE-g9lJb1jn"
      },
      "source": [
        "test_dataset = datasets.MNIST('./data/MNIST', train=False, download=True, transform=transform)\n",
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd7kL6yncInv"
      },
      "source": [
        "train_dataset[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4-hNP8EcPG3"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "DataLoader?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiVthmtOcyCh"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=256, pin_memory=True, shuffle=True)\n",
        "train_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0WMRT_NdBrU"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256, pin_memory=True, shuffle=False)\n",
        "test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYQxFSZdLz_"
      },
      "source": [
        "imgs, labels = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn7DjCJpdRI0"
      },
      "source": [
        "imgs.shape, labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjXEi1rQdRxI"
      },
      "source": [
        "output = model(imgs)\n",
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qiQRCx9drE7"
      },
      "source": [
        "idx = 10\n",
        "actual = labels[idx].item()\n",
        "print(actual)\n",
        "show_image(imgs[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nE8EBtOd41P"
      },
      "source": [
        "softmax = output[idx].exp()/output[idx].exp().sum()\n",
        "softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Jax1gkera-"
      },
      "source": [
        "predicted = softmax.argmax().item() \n",
        "print(f\"predicted={predicted}, actual={actual}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nzM4CmeohBq"
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "Let's use the standard cross-entropy loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlI_11le-Fg"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wui1FcHLpVzh"
      },
      "source": [
        "# create a fresh instance of your model \n",
        "model = MyNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClfvAHk1op_N"
      },
      "source": [
        "# define loss function (criterion)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjo59-9bpO3X"
      },
      "source": [
        "# pass some images through your model, get the outputs\n",
        "# why is the output 256 x 10?\n",
        "imgs, labels = next(iter(train_loader))\n",
        "output = model(imgs)\n",
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhqoM-EJpbVo"
      },
      "source": [
        "loss = criterion(output, labels)\n",
        "loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rALwxLI5pk2u"
      },
      "source": [
        "## Define the Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUtgJHR6pC3X"
      },
      "source": [
        "# define the optimizer\n",
        "# this updates the weights for us using gradient descent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40uAyAqNpwNn"
      },
      "source": [
        "## The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgTfJ9gCqbRn"
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, mb=None):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  criterion.to(device)\n",
        "\n",
        "  losses = []\n",
        "  for imgs,labels in progress_bar(train_loader, parent=mb):\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass \n",
        "    output = model(imgs)\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    # backward pass (compute gradients, do backprop)\n",
        "    optimizer.zero_grad() # zero out any existing gradients\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  return torch.tensor(losses).mean().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccuTg-tI5Tx6"
      },
      "source": [
        "## The \"test\" or \"validation\" loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tsz2THf5SXL"
      },
      "source": [
        "def validate(model, test_loader, criterion, optimizer, mb=None):\n",
        "  # use gpu if available\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.to(device)\n",
        "  criterion.to(device)\n",
        "\n",
        "  # place the model in \"eval\" mode (do not compute gradients during testing) \n",
        "  model.train()  \n",
        "\n",
        "  # iterate over batches, compute loss and accuracy for each batch\n",
        "  losses = []\n",
        "  correct = []\n",
        "  for imgs,labels in progress_bar(test_loader, parent=mb):\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass \n",
        "    output = model(imgs)\n",
        "\n",
        "    # calculate loss and classification accuracy\n",
        "    loss = criterion(output, labels)\n",
        "    _, correct_k = accuracy(output, labels, topk=(1,))             \n",
        "\n",
        "    losses.append(loss.item())\n",
        "    correct.append(correct_k)\n",
        "\n",
        "  top1 = torch.cat(correct).mean()\n",
        "\n",
        "  return torch.tensor(losses).mean().item(), top1.mean().item()\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        acc = []\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float()\n",
        "            acc.append(correct_k)            \n",
        "            res.append(correct_k.sum(0, keepdim=True).mul_(100.0 / batch_size))\n",
        "        return res, acc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPNfzkMt6y5X"
      },
      "source": [
        "# val_loss, top1 = validate(model, test_loader, criterion, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjPW7CSZpsww"
      },
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar \n",
        "num_epochs = 10\n",
        "\n",
        "mb = master_bar( range(num_epochs) )\n",
        "mb.names = ['train_loss', 'val_loss']\n",
        "xs,y1,y2 = [], [], []\n",
        "for epoch in mb:\n",
        "  train_loss = train(model, train_loader, criterion, optimizer, mb=mb)\n",
        "  val_loss, top1 = validate(model, test_loader, criterion, optimizer, mb=mb)\n",
        "  # print(f\"Epoch {epoch}: Train Loss {train_loss}, Val Loss {val_loss} Top1 {top1}\")\n",
        "\n",
        "  # graph results\n",
        "  xs.append(epoch)\n",
        "  y1.append(train_loss)\n",
        "  y2.append(val_loss)\n",
        "  graphs = [[xs,y1], [xs,y2]]\n",
        "  x_bounds = [0, num_epochs]\n",
        "  y_bounds = [0,2]\n",
        "  mb.update_graph(graphs, x_bounds, y_bounds)\n",
        "print(\"All Done!\")\n",
        "print(f\"Epoch {epoch}: Train Loss {train_loss}, Val Loss {val_loss} Top1 {top1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0mehlE6sXxF"
      },
      "source": [
        "show_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlt2N1_0ptPT"
      },
      "source": [
        "## Exercise 1 - Improve your Model by training longer on the GPU (e.g., compare peroformance for 10 epochs vs. 30 epochs)\n",
        "\n",
        "Goto \"Runtime\", select \"runtime type\" and choose \"GPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sorjd1C-AcHF"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wViylh9N_kDg"
      },
      "source": [
        "from fastprogress.fastprogress import master_bar, progress_bar \n",
        "\n",
        "def train_model(num_epochs):\n",
        "  mb = master_bar( range(num_epochs) )\n",
        "  mb.names = ['train_loss', 'val_loss']\n",
        "  xs,y1,y2 = [], [], []\n",
        "  for epoch in mb:\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, mb=mb)\n",
        "    val_loss, top1 = validate(model, test_loader, criterion, optimizer, mb=mb)\n",
        "    # print(f\"Epoch {epoch}: Train Loss {train_loss}, Val Loss {val_loss} Top1 {top1}\")\n",
        "\n",
        "    # graph results\n",
        "    xs.append(epoch)\n",
        "    y1.append(train_loss)\n",
        "    y2.append(val_loss)\n",
        "    graphs = [[xs,y1], [xs,y2]]\n",
        "    x_bounds = [0, num_epochs]\n",
        "    y_bounds = [0,max(max(y1),max(y2))*1.1]\n",
        "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
        "  print(\"All Done!\")\n",
        "  print(f\"Epoch {epoch}: Train Loss {train_loss}, Val Loss {val_loss} Top1 {top1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcimAPEU_uNA"
      },
      "source": [
        "model = MyNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.03)\n",
        "train_model(num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxdx02CFAgKs"
      },
      "source": [
        "model = MyNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.03)\n",
        "train_model(num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VksX0TAM-gs8"
      },
      "source": [
        "## Exercise 2 - Improve your Model by using a better optimizer (e.g., Adam, Adadelta), or by varying the learning rate, or both; \n",
        "\n",
        "Save a record of the results for each variant you try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb9vC7nPBYV6"
      },
      "source": [
        "model = MyNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=.03)\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
        "train_model(num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI4Cqv7LHe61"
      },
      "source": [
        "# show_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XfCU5bv-4gW"
      },
      "source": [
        "## Exercise 3 - Improve your Model by adding one or more hidden layers, with or without ReLU activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPOz9jsfCJ7B"
      },
      "source": [
        "class MyNetShallow(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyNet, self).__init__()\n",
        "    # in_features = 784, because the input image is 1x28x28 = 784\n",
        "    # out_features = 10, because there are 10 output categories (digits 0-9)\n",
        "    self.fc = nn.Linear(in_features=784, out_features=10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # in the \"forward pass\", we take an input (a batch of images, x)\n",
        "    # then first we flatten it into batchSize x 784, \n",
        "    batchSize = x.shape[0] # first dimension of x is \"batchSize\"\n",
        "    x = x.view(batchSize, -1) # the -1 tells pytorch to flatten the tensor to be batchSize x \"whatever size fits\"\n",
        "\n",
        "    # finally, we pass the flattened input into our fully-connected layer \n",
        "    # which will compute the weighted sum of the input for each of the 10 \n",
        "    # categories\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUw9zLJW_JyL"
      },
      "source": [
        "## Exercise 4 - Improve your Model by using convolutional layers\n",
        "\n",
        "Save a record of the results for each variant you try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESvsaVBfCVmr"
      },
      "source": [
        "from collections import OrderedDict\n",
        "# reference: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.cnn_backbone = nn.Sequential(OrderedDict([\n",
        "             ('conv1', nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)),\n",
        "             ('relu1', nn.ReLU()),\n",
        "             ('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)),\n",
        "             ('relu2', nn.ReLU()),\n",
        "             ('pool2', nn.MaxPool2d(2)),\n",
        "             ('dropout2', nn.Dropout2d(0.25))\n",
        "        ]))\n",
        "        self.head = nn.Sequential(OrderedDict([\n",
        "            ('fc3', nn.Linear(9216, 128)),\n",
        "            ('relu3', nn.ReLU()),\n",
        "            ('dropout3', nn.Dropout2d(0.50)),\n",
        "            ('fc4', nn.Linear(128, 10)),\n",
        "            ('relu4', nn.ReLU()),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_backbone(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMOvuV3fCcia"
      },
      "source": [
        "model = CNN()\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=.03)\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
        "train_model(num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tHUn6Lm_UyR"
      },
      "source": [
        "## Exercise 5 - Challenge your model by adding position and scale variation, see how this effects learning, generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6cXqUMgHMBO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9mGiT7N-wBb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6GqjX70oYLu"
      },
      "source": [
        ""
      ]
    }
  ]
}