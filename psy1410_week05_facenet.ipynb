{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "psy1410_week05_facenet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "SBj8eBseC1s8",
        "tf1mYr8zBz4D",
        "34CjKpaODXZI"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB2UrzUjUJXTVNsbvSRZ//",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvard-visionlab/psy1410/blob/master/psy1410_week05_facenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhLtlVVSBgg3"
      },
      "source": [
        "# Overview - FaceNet Notebook!Â¶\n",
        "This notebook will allow you to probe the representations of a deep, convolutional neural network trained to represent faces (FaceNet).\n",
        "Specifically, you can upload images, pass them through your deep neural network, and then evaluate how similar the images are in the low-dimensional \"embedding space\" of the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBj8eBseC1s8"
      },
      "source": [
        "# Install Python Packages\n",
        "\n",
        "- you only need to run this once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgEOuxGWC1Jd"
      },
      "source": [
        "!pip install facenet-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf1mYr8zBz4D"
      },
      "source": [
        "# Download Some Images\n",
        "\n",
        "- You only need to do this once if you don't see these images in your \"images\" folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czYjxk-RBzRZ"
      },
      "source": [
        "!mkdir -p images\n",
        "!wget -c https://www.dropbox.com/s/7tqlvb69lvx570h/BaldBear.jpg -q --show-progress -O /content/images/BaldBear.jpg\n",
        "!wget -c https://www.dropbox.com/s/lv6tjxuc90mksgc/BrownBear.jpg -q --show-progress -O /content/images/BrownBear.jpg\n",
        "!wget -c https://www.dropbox.com/s/65p68g331kby809/Gorilla.jpg -q --show-progress -O /content/images/Gorilla.jpg\n",
        "!wget -c https://www.dropbox.com/s/be1hkifaz8u04y9/DiCaprio_Anchor.jpg -q --show-progress -O /content/images/DiCaprio_Anchor.jpg\n",
        "!wget -c https://www.dropbox.com/s/xn3y46bpccopdl7/DiCaprio_HardNegative.jpg -q --show-progress -O /content/images/DiCaprio_HardNegative.jpg\n",
        "!wget -c https://www.dropbox.com/s/8londclzzyj3oji/DiCaprio_NegativeClooney.jpg -q --show-progress -O /content/images/DiCaprio_NegativeClooney.jpg\n",
        "!wget -c https://www.dropbox.com/s/ddlfya3368jdhci/DiCaprio_Positive.jpg -q --show-progress -O /content/images/DiCaprio_Positive.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTNxnbd9BuZb"
      },
      "source": [
        "# Imports\n",
        "- run this cell to import all the helpers you need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q5T2qwCBWwB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "%config InlineBackend.figure_format='retina'\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "sns.set(rc={'figure.figsize':(15.7,8.27)})\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "class ImageListDataset(Dataset):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, imgs, transform=None):\n",
        "        self.root_dir = None\n",
        "        self.files = imgs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.files[index])\n",
        "        if self.transform:\n",
        "            im = self.transform(im)\n",
        "\n",
        "        return im, 0, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __repr__(self):\n",
        "        _repr_indent = 4\n",
        "        head = \"Dataset \" + self.__class__.__name__\n",
        "        body = [\"Number of Images: {}\".format(self.__len__())]\n",
        "        if self.root_dir is not None:\n",
        "            body.append(\"Root location: {}\".format(self.root_dir))\n",
        "        if hasattr(self, \"transform\") and self.transform is not None:\n",
        "            body += [repr(self.transform)]\n",
        "        lines = [head] + [\" \" * _repr_indent + line for line in body]\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "def get_dataset(image_pairs, root_dir=Path('images')):\n",
        "    root_dir = Path(root_dir)\n",
        "    transform = transforms.Compose([\n",
        "        lambda x: x.convert('RGB'),\n",
        "        transforms.Resize((224, 224)),\n",
        "    ])\n",
        "    imgs = [root_dir/img for imgs in image_pairs for img in imgs]\n",
        "    dataset = ImageListDataset(imgs=imgs, transform=transform)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def show_grid(dataset):\n",
        "    imgs = []\n",
        "    for image_num in range(0, len(dataset), 2):\n",
        "        imgs.append(np.hstack(\n",
        "            [np.array(dataset[image_num][0]), np.array(dataset[image_num+1][0])]))\n",
        "    imgs = np.vstack(imgs)\n",
        "\n",
        "    return Image.fromarray(imgs)\n",
        "\n",
        "'''\n",
        "Utilities for instrumenting a torch model.\n",
        "InstrumentedModel will wrap a pytorch model and allow hooking\n",
        "arbitrary layers to monitor or modify their output directly.\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import types\n",
        "import copy\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class InstrumentedModel(torch.nn.Module):\n",
        "    '''\n",
        "    A wrapper for hooking, probing and intervening in pytorch Modules.\n",
        "    Example usage:\n",
        "    ```\n",
        "    model = load_my_model()\n",
        "    with inst as InstrumentedModel(model):\n",
        "        inst.retain_layer(layername)\n",
        "        inst.edit_layer(layername, ablation=0.5, replacement=target_features)\n",
        "        inst(inputs)\n",
        "        original_features = inst.retained_layer(layername)\n",
        "    ```\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self._retained = OrderedDict()\n",
        "        self._detach_retained = {}\n",
        "        self._editargs = defaultdict(dict)\n",
        "        self._editrule = {}\n",
        "        self._hooked_layer = {}\n",
        "        self._old_forward = {}\n",
        "        if isinstance(model, torch.nn.Sequential):\n",
        "            self._hook_sequential()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.close()\n",
        "\n",
        "    def forward(self, *inputs, **kwargs):\n",
        "        return self.model(*inputs, **kwargs)\n",
        "\n",
        "    def retain_layer(self, layername, detach=True):\n",
        "        '''\n",
        "        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n",
        "        to hook that layer and retain its output each time the model is run.\n",
        "        A pair (layername, aka) can be provided, and the aka will be used\n",
        "        as the key for the retained value instead of the layername.\n",
        "        '''\n",
        "        self.retain_layers([layername], detach=detach)\n",
        "\n",
        "    def retain_layers(self, layernames, detach=True):\n",
        "        '''\n",
        "        Retains a list of a layers at once.\n",
        "        '''\n",
        "        self.add_hooks(layernames)\n",
        "        for layername in layernames:\n",
        "            aka = layername\n",
        "            if not isinstance(aka, str):\n",
        "                layername, aka = layername\n",
        "            if aka not in self._retained:\n",
        "                self._retained[aka] = None\n",
        "                self._detach_retained[aka] = detach\n",
        "\n",
        "    def stop_retaining_layers(self, layernames):\n",
        "        '''\n",
        "        Removes a list of layers from the set retained.\n",
        "        '''\n",
        "        self.add_hooks(layernames)\n",
        "        for layername in layernames:\n",
        "            aka = layername\n",
        "            if not isinstance(aka, str):\n",
        "                layername, aka = layername\n",
        "            if aka in self._retained:\n",
        "                del self._retained[aka]\n",
        "                del self._detach_retained[aka]\n",
        "\n",
        "    def retained_features(self, clear=False):\n",
        "        '''\n",
        "        Returns a dict of all currently retained features.\n",
        "        '''\n",
        "        result = OrderedDict(self._retained)\n",
        "        if clear:\n",
        "            for k in result:\n",
        "                self._retained[k] = None\n",
        "        return result\n",
        "\n",
        "    def retained_layer(self, aka=None, clear=False):\n",
        "        '''\n",
        "        Retrieve retained data that was previously hooked by retain_layer.\n",
        "        Call this after the model is run.  If clear is set, then the\n",
        "        retained value will return and also cleared.\n",
        "        '''\n",
        "        if aka is None:\n",
        "            # Default to the first retained layer.\n",
        "            aka = next(self._retained.keys().__iter__())\n",
        "        result = self._retained[aka]\n",
        "        if clear:\n",
        "            self._retained[aka] = None\n",
        "        return result\n",
        "\n",
        "    def edit_layer(self, layername, rule=None, **kwargs):\n",
        "        '''\n",
        "        Pass a fully-qualified layer name (E.g., module.submodule.conv3)\n",
        "        to hook that layer and modify its output each time the model is run.\n",
        "        The output of the layer will be modified to be a convex combination\n",
        "        of the replacement and x interpolated according to the ablation, i.e.:\n",
        "        `output = x * (1 - a) + (r * a)`.\n",
        "        '''\n",
        "        if not isinstance(layername, str):\n",
        "            layername, aka = layername\n",
        "        else:\n",
        "            aka = layername\n",
        "\n",
        "        # The default editing rule is apply_ablation_replacement\n",
        "        if rule is None:\n",
        "            rule = apply_ablation_replacement\n",
        "\n",
        "        self.add_hooks([(layername, aka)])\n",
        "        self._editargs[aka].update(kwargs)\n",
        "        self._editrule[aka] = rule\n",
        "\n",
        "    def remove_edits(self, layername=None):\n",
        "        '''\n",
        "        Removes edits at the specified layer, or removes edits at all layers\n",
        "        if no layer name is specified.\n",
        "        '''\n",
        "        if layername is None:\n",
        "            self._editargs.clear()\n",
        "            self._editrule.clear()\n",
        "            return\n",
        "\n",
        "        if not isinstance(layername, str):\n",
        "            layername, aka = layername\n",
        "        else:\n",
        "            aka = layername\n",
        "        if aka in self._editargs:\n",
        "            del self._editargs[aka]\n",
        "        if aka in self._editrule:\n",
        "            del self._editrule[aka]\n",
        "\n",
        "    def add_hooks(self, layernames):\n",
        "        '''\n",
        "        Sets up a set of layers to be hooked.\n",
        "        Usually not called directly: use edit_layer or retain_layer instead.\n",
        "        '''\n",
        "        needed = set()\n",
        "        aka_map = {}\n",
        "        for name in layernames:\n",
        "            aka = name\n",
        "            if not isinstance(aka, str):\n",
        "                name, aka = name\n",
        "            if self._hooked_layer.get(aka, None) != name:\n",
        "                aka_map[name] = aka\n",
        "                needed.add(name)\n",
        "        if not needed:\n",
        "            return\n",
        "        for name, layer in self.model.named_modules():\n",
        "            if name in aka_map:\n",
        "                needed.remove(name)\n",
        "                aka = aka_map[name]\n",
        "                self._hook_layer(layer, name, aka)\n",
        "        for name in needed:\n",
        "            raise ValueError('Layer %s not found in model' % name)\n",
        "\n",
        "    def _hook_layer(self, layer, layername, aka):\n",
        "        '''\n",
        "        Internal method to replace a forward method with a closure that\n",
        "        intercepts the call, and tracks the hook so that it can be reverted.\n",
        "        '''\n",
        "        if aka in self._hooked_layer:\n",
        "            raise ValueError('Layer %s already hooked' % aka)\n",
        "        if layername in self._old_forward:\n",
        "            raise ValueError('Layer %s already hooked' % layername)\n",
        "        self._hooked_layer[aka] = layername\n",
        "        self._old_forward[layername] = (layer, aka,\n",
        "                                        layer.__dict__.get('forward', None))\n",
        "        editor = self\n",
        "        original_forward = layer.forward\n",
        "\n",
        "        def new_forward(self, *inputs, **kwargs):\n",
        "            original_x = original_forward(*inputs, **kwargs)\n",
        "            x = editor._postprocess_forward(original_x, aka)\n",
        "            return x\n",
        "        layer.forward = types.MethodType(new_forward, layer)\n",
        "\n",
        "    def _unhook_layer(self, aka):\n",
        "        '''\n",
        "        Internal method to remove a hook, restoring the original forward method.\n",
        "        '''\n",
        "        if aka not in self._hooked_layer:\n",
        "            return\n",
        "        layername = self._hooked_layer[aka]\n",
        "        # Remove any retained data and any edit rules\n",
        "        if aka in self._retained:\n",
        "            del self._retained[aka]\n",
        "            del self._detach_retained[aka]\n",
        "        self.remove_edits(aka)\n",
        "        # Restore the unhooked method for the layer\n",
        "        layer, check, old_forward = self._old_forward[layername]\n",
        "        assert check == aka\n",
        "        if old_forward is None:\n",
        "            if 'forward' in layer.__dict__:\n",
        "                del layer.__dict__['forward']\n",
        "        else:\n",
        "            layer.forward = old_forward\n",
        "        del self._old_forward[layername]\n",
        "        del self._hooked_layer[aka]\n",
        "\n",
        "    def _postprocess_forward(self, x, aka):\n",
        "        '''\n",
        "        The internal method called by the hooked layers after they are run.\n",
        "        '''\n",
        "        # Retain output before edits, if desired.\n",
        "        if aka in self._retained:\n",
        "            if self._detach_retained[aka]:\n",
        "                self._retained[aka] = x.detach()\n",
        "            else:\n",
        "                self._retained[aka] = x\n",
        "        # Apply any edits requested.\n",
        "        rule = self._editrule.get(aka, None)\n",
        "        if rule is not None:\n",
        "            x = rule(x, self, **(self._editargs[aka]))\n",
        "        return x\n",
        "\n",
        "    def _hook_sequential(self):\n",
        "        '''\n",
        "        Replaces 'forward' of sequential with a version that takes\n",
        "        additional keyword arguments: layer allows a single layer to be run;\n",
        "        first_layer and last_layer allow a subsequence of layers to be run.\n",
        "        '''\n",
        "        model = self.model\n",
        "        self._hooked_layer['.'] = '.'\n",
        "        self._old_forward['.'] = (model, '.',\n",
        "                                  model.__dict__.get('forward', None))\n",
        "\n",
        "        def new_forward(this, x, layer=None, first_layer=None, last_layer=None):\n",
        "            assert layer is None or (\n",
        "                first_layer is None and last_layer is None)\n",
        "            first_layer, last_layer = [str(layer) if layer is not None\n",
        "                                       else str(d) if d is not None else None\n",
        "                                       for d in [first_layer, last_layer]]\n",
        "            including_children = (first_layer is None)\n",
        "            for name, layer in this._modules.items():\n",
        "                if name == first_layer:\n",
        "                    first_layer = None\n",
        "                    including_children = True\n",
        "                if including_children:\n",
        "                    x = layer(x)\n",
        "                if name == last_layer:\n",
        "                    last_layer = None\n",
        "                    including_children = False\n",
        "            assert first_layer is None, '%s not found' % first_layer\n",
        "            assert last_layer is None, '%s not found' % last_layer\n",
        "            return x\n",
        "        model.forward = types.MethodType(new_forward, model)\n",
        "\n",
        "    def close(self):\n",
        "        '''\n",
        "        Unhooks all hooked layers in the model.\n",
        "        '''\n",
        "        for aka in list(self._old_forward.keys()):\n",
        "            self._unhook_layer(aka)\n",
        "        assert len(self._old_forward) == 0\n",
        "\n",
        "\n",
        "def apply_ablation_replacement(x, imodel, **buffers):\n",
        "    if buffers is not None:\n",
        "        # Apply any edits requested.\n",
        "        a = make_matching_tensor(buffers, 'ablation', x)\n",
        "        if a is not None:\n",
        "            x = x * (1 - a)\n",
        "            v = make_matching_tensor(buffers, 'replacement', x)\n",
        "            if v is not None:\n",
        "                x += (v * a)\n",
        "    return x\n",
        "\n",
        "\n",
        "def make_matching_tensor(valuedict, name, data):\n",
        "    '''\n",
        "    Converts `valuedict[name]` to be a tensor with the same dtype, device,\n",
        "    and dimension count as `data`, and caches the converted tensor.\n",
        "    '''\n",
        "    v = valuedict.get(name, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    if not isinstance(v, torch.Tensor):\n",
        "        # Accept non-torch data.\n",
        "        v = torch.from_numpy(numpy.array(v))\n",
        "        valuedict[name] = v\n",
        "    if not v.device == data.device or not v.dtype == data.dtype:\n",
        "        # Ensure device and type matches.\n",
        "        assert not v.requires_grad, '%s wrong device or type' % (name)\n",
        "        v = v.to(device=data.device, dtype=data.dtype)\n",
        "        valuedict[name] = v\n",
        "    if len(v.shape) < len(data.shape):\n",
        "        # Ensure dimensions are unsqueezed as needed.\n",
        "        assert not v.requires_grad, '%s wrong dimensions' % (name)\n",
        "        v = v.view((1,) + tuple(v.shape) +\n",
        "                   (1,) * (len(data.shape) - len(v.shape) - 1))\n",
        "        valuedict[name] = v\n",
        "    return v\n",
        "\n",
        "\n",
        "def subsequence(sequential, first_layer=None, last_layer=None,\n",
        "                share_weights=False):\n",
        "    '''\n",
        "    Creates a subsequence of a pytorch Sequential model, copying over\n",
        "    modules together with parameters for the subsequence.  Only\n",
        "    modules from first_layer to last_layer (inclusive) are included.\n",
        "    If share_weights is True, then references the original modules\n",
        "    and their parameters without copying them.  Otherwise, by default,\n",
        "    makes a separate brand-new copy.\n",
        "    '''\n",
        "    included_children = OrderedDict()\n",
        "    including_children = (first_layer is None)\n",
        "    for name, layer in sequential._modules.items():\n",
        "        if name == first_layer:\n",
        "            first_layer = None\n",
        "            including_children = True\n",
        "        if including_children:\n",
        "            included_children[name] = layer if share_weights else (\n",
        "                copy.deepcopy(layer))\n",
        "        if name == last_layer:\n",
        "            last_layer = None\n",
        "            including_children = False\n",
        "    if first_layer is not None:\n",
        "        raise ValueError('Layer %s not found' % first_layer)\n",
        "    if last_layer is not None:\n",
        "        raise ValueError('Layer %s not found' % last_layer)\n",
        "    if not len(included_children):\n",
        "        raise ValueError('Empty subsequence')\n",
        "    return torch.nn.Sequential(OrderedDict(included_children))\n",
        "\n",
        "    import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from pprint import pprint\n",
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "alexnet_pytorch_blocks = OrderedDict([\n",
        "    ('Conv1', ['features.0','features.1','features.2']),\n",
        "    ('Conv2', ['features.3','features.4','features.5']),\n",
        "    ('Conv3', ['features.6','features.7']),\n",
        "    ('Conv4', ['features.8','features.9']),\n",
        "    ('Conv5', ['features.10','features.11','features.12']),\n",
        "    ('', ['avgpool']),\n",
        "    ('fc6', ['classifier.0','classifier.1','classifier.2']),\n",
        "    ('fc7', ['classifier.3','classifier.4','classifier.5']),\n",
        "    ('fc8', ['classifier.6']),\n",
        "])\n",
        "\n",
        "def plot_results(df):\n",
        "    pair_names = []\n",
        "    for i, row in df.iterrows():\n",
        "        img1 = row.image1.replace(\".jpg\",\"\").replace(\".png\",\"\").replace(\".tiff\",\"\")\n",
        "        img2 = row.image2.replace(\".jpg\",\"\").replace(\".png\",\"\").replace(\".tiff\",\"\")\n",
        "        pair_name = img1 + \"_\" + img2\n",
        "        pair_names.append(pair_name)\n",
        "    df['pair_name'] = pair_names    \n",
        "    ax = sns.barplot(x=\"pair_name\", y=\"euclidean_distance\", data=df)\n",
        "    ax.set_title(\"Euclidean Distance Between Pairs (larger = more different)\", fontsize=20)\n",
        "    return ax;\n",
        "\n",
        "def plot_df(df, pairs=[0,1,2], title='', blocks=None, legend_loc=(0.25, 0.80), group_by='pair_num', ceiling=1, ylabel='correlation', legend_color=(0.95,0.95,0.95,1.0)):\n",
        "    \n",
        "    if pairs is None:\n",
        "        #ax = plot_data(df, title, ymax=1.10, ymin=-0.20, hue=group_by, ylabel=ylabel)\n",
        "        ax = plot_data(df[df.pair_num.isin(pairs)], title, ymax=1.10, ymin=-0.20, hue=group_by, ylabel=ylabel)\n",
        "    else:\n",
        "        ax = plot_data(df[df.pair_num.isin(pairs)], title, ymax=1.10, ymin=-0.20, hue=group_by, ylabel=ylabel)\n",
        "    \n",
        "    if blocks:\n",
        "        draw_bg(blocks, ypos=1.03, legend_loc=legend_loc)\n",
        "    \n",
        "    L = ax.legend()\n",
        "    legend_labels = ['image_pair']        \n",
        "    \n",
        "    for pair in pairs:\n",
        "        label = df[df.pair_num == pair].iloc[0].image1.replace('.jpg', '') + '_vs_' + df[df.pair_num == pair].iloc[0].image2.replace('.jpg', '')\n",
        "        legend_labels.append(label)        \n",
        "    for label_num, label in enumerate(legend_labels):\n",
        "        if label is not None: L.get_texts()[label_num].set_text(label)\n",
        "\n",
        "    L.set_bbox_to_anchor(legend_loc)    \n",
        "    \n",
        "    return ax\n",
        "\n",
        "def plot_data(df, title, ymax=.50, ymin=0.0, hue=None, ylabel='correlation'):\n",
        "    sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
        "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "    ax = sns.lineplot(x=\"layer\", y=\"y\", hue=hue, data=df, linewidth=2)\n",
        "    ax.set_title(title, fontsize=24);\n",
        "    ax.set_ylabel(ylabel, fontsize=24, labelpad=15);\n",
        "    ax.set_xlabel(\"layer\", fontsize=24, labelpad=20);\n",
        "    ax.set_ylim([ymin, ymax])\n",
        "    plt.xticks(rotation=90);\n",
        "    \n",
        "    return ax\n",
        "\n",
        "def draw_bg(blocks, ypos=0.475, alpha_b=.20, alpha_g=.15, legend_loc=(0.79, 0.80)):\n",
        "    if blocks == None: return\n",
        "    c = 0\n",
        "    for idx, (block_name, layers) in enumerate(blocks.items()):\n",
        "        n_layers = len(layers)\n",
        "        for i in range(c, c+n_layers):\n",
        "            if idx % 2 == 0:\n",
        "                plt.axvspan(i-.5, i+.5, facecolor='b', alpha=alpha_b, lw=0)\n",
        "            else:\n",
        "                plt.axvspan(i-.5, i+.5, facecolor='gray', alpha=alpha_g, lw=0)\n",
        "        plt.text(c+(n_layers)/2-.5, ypos, block_name, fontdict=None, fontsize=16, ha='center', va='center')\n",
        "        c += n_layers\n",
        "    plt.legend(facecolor=(0.95,0.95,0.95,1.0), bbox_to_anchor=legend_loc) \n",
        "    \n",
        "def plot(df, legend_loc=(0.25, 0.70)):\n",
        "    df['y'] = df['r']\n",
        "    layer_name = lambda x: \"{:02d}_{}\".format(x.layer_num,x.layer_type.replace(\"BatchNorm2d\",\"Norm\").replace(\"GroupNorm\", \"Norm\"))\n",
        "    df['layer'] = df[['layer_num','layer_type']].apply(layer_name, axis=1)\n",
        "    \n",
        "    blocks = alexnet_pytorch_blocks\n",
        "    \n",
        "    pairs = df.pair_num.unique()\n",
        "    \n",
        "    ax = plot_df(df, blocks=blocks, pairs=pairs, legend_loc=legend_loc)\n",
        "\n",
        "    return ax\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from fastprogress import master_bar, progress_bar\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "tfrm = transforms.Compose([\n",
        "    transforms.CenterCrop(160),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def compute_embeddings(dataset):\n",
        "    print(f\"Computing Embeddings (N={len(dataset)} images)\")\n",
        "    cache = {}\n",
        "    mtcnn = MTCNN(image_size=160)\n",
        "    resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "    \n",
        "    embeddings = []\n",
        "    embedding = []\n",
        "    for idx, (img, label, index) in enumerate(progress_bar(dataset)):\n",
        "        \n",
        "        # Get cropped and prewhitened image tensor\n",
        "        img_cropped = None\n",
        "        try:\n",
        "          img_cropped = mtcnn(img)\n",
        "        except:\n",
        "          pass \n",
        "          \n",
        "        if img_cropped is None:\n",
        "            print(\"Warning, no human face detected, using center crop:\", dataset.files[idx])\n",
        "            img_cropped = tfrm(img)\n",
        "            \n",
        "        # Calculate embedding (unsqueeze to add batch dimension)\n",
        "        img_embedding = resnet(img_cropped.unsqueeze(0))\n",
        "        \n",
        "        embedding.append(img_embedding)\n",
        "        \n",
        "        if len(embedding) == 2:\n",
        "            embeddings.append(embedding)\n",
        "            embedding = []\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "def compare_embeddings(embeddings, image_pairs):\n",
        "    df = pd.DataFrame(columns=['pair_num','image1','image2','euclidean_distance'])\n",
        "    \n",
        "    for pair_num, ((embed1, embed2), (image1, image2)) in enumerate(zip(embeddings, image_pairs)):\n",
        "        df = df.append({\n",
        "            \"pair_num\": pair_num,\n",
        "            \"image1\": image1,\n",
        "            \"image2\": image2, \n",
        "            \"euclidean_distance\": (embed1-embed2).pow(2).sum().item()\n",
        "        }, ignore_index=True)\n",
        "        \n",
        "    return df\n",
        "\n",
        "def get_layer(m, layers):\n",
        "    layer = layers.pop(0)\n",
        "    m = getattr(m, layer)\n",
        "    if len(layers) > 0:\n",
        "        return get_layer(m, layers)\n",
        "    return m\n",
        "\n",
        "\n",
        "def get_layers(model, parent_name='', layer_info=[]):\n",
        "    for module_name, module in model.named_children():\n",
        "        layer_name = parent_name + '.' + module_name\n",
        "        if len(list(module.named_children())):\n",
        "            layer_info = get_layers(module, layer_name, layer_info=layer_info)\n",
        "        else:\n",
        "            layer_info.append(layer_name.strip('.'))\n",
        "\n",
        "    return layer_info\n",
        "\n",
        "\n",
        "def get_layer_type(model, layer_name):\n",
        "    m = get_layer(model, layer_name.split(\".\"))\n",
        "    return m.__class__.__name__\n",
        "\n",
        "\n",
        "def convert_relu_layers(parent):\n",
        "    for child_name, child in parent.named_children():\n",
        "        if isinstance(child, nn.ReLU):\n",
        "            setattr(parent, child_name, nn.ReLU(inplace=False))\n",
        "        elif len(list(child.children())) > 0:\n",
        "            convert_relu_layers(child)\n",
        "\n",
        "\n",
        "def store_activations(model, layer_names):\n",
        "    a = OrderedDict()\n",
        "    for layer_num, layer_name in enumerate(layer_names):\n",
        "        layer_type = get_layer_type(model.model, layer_name)\n",
        "        X = model.retained_layer(layer_name)\n",
        "        X = X.view(X.shape[0], -1)\n",
        "        a[layer_name] = X\n",
        "    return a\n",
        "\n",
        "\n",
        "def compute_similarity(model, dataset):\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # hook model\n",
        "    layer_names = get_layers(model, parent_name='', layer_info=[])\n",
        "    if not isinstance(model, nethook.InstrumentedModel):\n",
        "        model = nethook.InstrumentedModel(model)\n",
        "    for layer_name in layer_names:\n",
        "        model.retain_layer(layer_name)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # create dataloader\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = ImageListDataset(imgs=dataset.files, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=1,\n",
        "                            shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    # compute similarity by layer\n",
        "    df = pd.DataFrame(columns=['pair_num', 'image1', 'image2',\n",
        "                               'layer_num', 'layer_name', 'layer_type', 'r'])\n",
        "    pair_num = 0\n",
        "    mb = master_bar(dataloader)\n",
        "    for count, (imgs, labels, indexes) in enumerate(mb):\n",
        "        with torch.no_grad():\n",
        "            model(imgs.to(device))\n",
        "        if count % 2 == 0:\n",
        "            a1 = store_activations(model, layer_names)\n",
        "            image1 = dataset.files[indexes].name\n",
        "        if count % 2 == 1:\n",
        "            a2 = store_activations(model, layer_names)\n",
        "            image2 = dataset.files[indexes].name\n",
        "            for layer_num, layer_name in enumerate(progress_bar(layer_names, parent=mb)):\n",
        "                r = pearsonr(a1[layer_name].squeeze(),\n",
        "                             a2[layer_name].squeeze())[0]\n",
        "                layer_type = get_layer_type(model.model, layer_name)\n",
        "                df = df.append({\n",
        "                    \"pair_num\": pair_num,\n",
        "                    \"image1\": image1,\n",
        "                    \"image2\": image2,\n",
        "                    \"layer_num\": layer_num,\n",
        "                    \"layer_name\": layer_name,\n",
        "                    \"layer_type\": layer_type,\n",
        "                    \"r\": r,\n",
        "                }, ignore_index=True)\n",
        "\n",
        "            pair_num += 1\n",
        "\n",
        "    df.pair_num = df.pair_num.astype(int)\n",
        "\n",
        "    return df        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34CjKpaODXZI"
      },
      "source": [
        "# Load FaceNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6ZPsbfCjvj"
      },
      "source": [
        "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zn5pqbEIov"
      },
      "source": [
        "# Exercise 1 - Compare Pairs of Faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2fXIJriELJ-"
      },
      "source": [
        "# here we define a list [ ... ]\n",
        "# with a comma-separated set of \"tuples\" ( ... )\n",
        "# that define pairs of images to be compared\n",
        "image_pairs = [\n",
        "    ('thatcher_normal_upright.jpg', 'thatcher_weird_upright.jpg'),\n",
        "    ('thatcher_normal_inverted.jpg', 'thatcher_weird_inverted.jpg'),\n",
        "    # ('DiCaprio_Anchor.jpg', 'DiCaprio_HardNegative.jpg')\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK2H_agBFKgn"
      },
      "source": [
        "# create a dataset from your list\n",
        "dataset = get_dataset(image_pairs, root_dir='/content/images')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4koZJEEJFOSu"
      },
      "source": [
        "# visualize your images\n",
        "show_grid(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rSJaKuRFRBH"
      },
      "source": [
        "# compute your embeddings\n",
        "# warning, this might be slow!\n",
        "embeddings = compute_embeddings(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snb0ddyFGoZr"
      },
      "source": [
        "# compare embeddings (compute eucldian distance between pairs)\n",
        "results = compare_embeddings(embeddings, image_pairs)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzyojaplGwIX"
      },
      "source": [
        "# plot the results\n",
        "plot_results(results);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzG5TL34HUis"
      },
      "source": [
        "## document your observations and conclusions\n",
        "***observations***  \n",
        "The Leo Anchor is more similar to the other Leo picture (DiCaprio_Positive), than to George Clooney (Negative Clooney), but NOT to the \"Hard Negative\"!!!\n",
        "\n",
        "***interpretation***   \n",
        "The network agrees that the \"Hard Negative\" Looks more like Leo than other people, or even other pictures of himself.\n",
        "\n",
        "***discussion***    \n",
        "Does this agree with your subjective impression?\n",
        "Definitely, but I wonder if the pose matters.\n",
        "\n",
        "***What conclusions would you draw about the network based on these results?***  \n",
        "I think this network does a pretty good job capturing face representations, but XYZ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-NOZBDMH0cA"
      },
      "source": [
        "# Exercise 2 - Perform stronger tests to see what's driving the pattern above.\n",
        "\n",
        "- [ ] how much does this depend on the \"head tilt\" matching?\n",
        "- [ ] try a variety of positives and negatives to test your intuitions/ideas\n",
        "- [ ] try different sets of images to see how well FaceNet performs in general (e.g., goto images.google.com and type \"totally looks like\" to find a bunch of \"hard negatives\"). \n",
        "- [ ] try an array of positives (from easy to hard) and an array of negatives (from easy to hard)...Does FaceNet always treat your positives as more similar than your negatives? When does it fail? Can you \"see why\" it failed (your best guess about which feature led to the error), and then add images to test your hypothesis?\n",
        "\n",
        "If you want to manipulate these images we downloaded to colab, you can download them (hover over the filename, see the three vertical dots to the right of the filename, click on it, and select download). Then you can edit them in whatever software you like (photoshop, preview), and save a copy with a different file name. Then upload the new files to your \"images\" folder just by dragging and dropping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyY1ifBCIt1a"
      },
      "source": [
        "#copy and paste steps from above to analyze your new pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9eXGr6fNuSZ"
      },
      "source": [
        "## document your observations and conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM9dWsSI1gT"
      },
      "source": [
        "# Exercise 3 - Test whether FaceNet shows 'holistic' processing\n",
        "\n",
        "- goto images.google.com and search \"composite face effect\" to find images you can use as stimuli, or you can extract them from the reading\n",
        "- you can use photoshop or other editing software to crop/flip/rotate images\n",
        "- make sure your images are \"square\" otherwise they will appear \"squished\" here where they are resized into a square"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLyzcMOuJBQL"
      },
      "source": [
        "#copy and paste steps from above to analyze your new pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXShfzN4Nyxs"
      },
      "source": [
        "## document your observations and conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwzWZQnoLjaS"
      },
      "source": [
        "# Exercise 4 - Does FaceNet show the Thatcher Illusion?\n",
        "\n",
        "Our perceptual system is \"extremely bothered\" by the inversion of features (bottom left vs bottom right), but really only when the faces are upright (bottom), suggesting this effect might depend on 'holistic processing'. Another way of describing this is that the upside down faces \"look more similar\" than the upright faces (where one is freakishly weird). Is this reflected in FaceNet responses?\n",
        "\n",
        "![alt text](https://images.ctfassets.net/cnu0m8re1exe/EbqlkHHcoQGIEMmtbMo17/d7f477b26ee0840dca9a526d12586ef9/thatcher_illusion.png?w=650 \"\")\n",
        "\n",
        "- you can find more examples of this effect if you goto images.google.com and search for \"thatcher illusion\"\n",
        "- It's always helpful to examine multiple examples so that you can be confident about how facenet handles this phenomenon in general\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aSj-RC5N05W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM-P-_efN1FA"
      },
      "source": [
        "## document your observations and conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNOdSV1KaXM"
      },
      "source": [
        "## Exercise 5 - Run your own tests\n",
        "\n",
        "You can try anything here. Do I look more like my mother or father? Do I really look like \"Celebrity X\"? \n",
        "\n",
        "***instructions***\n",
        "- choose an anchor image (\"me.jpg\")\n",
        "- choose positive and negative pairs (maybe some are \"hard negatives\", i.e., more like the anchor person than others to your eye; maybe some are hard positives, less obviously the same as the anchor person, but you are confident its the same person)\n",
        "- compare the distance to your \"positive\" and \"negative\" pairs. \n",
        "\n",
        "***record your observations***  \n",
        "Were you right about what would be a hard positive? Did the network fail on your hard positives (saying they are less like the anchor than some of your negatives)? etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32MGK-ziN2L9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_GKTBm5N2iV"
      },
      "source": [
        "## document your observations and conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4NtArc2N4mt"
      },
      "source": [
        "# Final thoughts?\n",
        "\n",
        "1. How \"good\" is the face representation of FaceNet? Where did it succeed, and where did it fail? \n",
        "\n",
        "2. How \"human-like\" is it's representational space (where we are treating \"holistic\" processing is a hallmark of human face representation)? \n",
        "\n",
        "3. Other observations, thoughts, or conclusions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX1LvC0oJBk_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}